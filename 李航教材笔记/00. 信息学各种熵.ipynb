{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 自信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "I(p_{i}) = - \\log (p_{i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 信息熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述自信息描述的是随机变量的某个事件发生所带来的信息量，而<font color=blue>信息熵通常用来描述整个随机分布所带来的信息平均值，更具统计特征。</font>  \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "H(X) &=& E_{x \\sim p}\\left[ I(x) \\right] = - E_{x \\sim p}\\left[ \\log p(x) \\right]  \\\\\n",
    "&=& - \\sum_{i=1}^{m}p(x_{i})\\log p(x_{i})  \\\\\n",
    "&=& \\int_{x}p(x)\\log p(x)\\mathrm{~d}x\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "熵代表了随机分布的混乱程度，这一特性是所有基于熵的机器学习算法的核心思想。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 条件熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在$X$给定条件下，$Y$的条件概率分布$P(Y|X)$的熵对$X$的数学期望。 \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\color{blue}{H(Y|X)} &=& \\color{blue}{E_{x\\sim p}\\left[ H(Y|X=x) \\right]} = \\sum_{i=1}^{n}p(x_{i})H(Y|X=x_{i}) \\\\\n",
    "&=& - \\sum_{i=1}^{n}p(x_{i}) \\sum_{j=1}^{m}p(y_{j}|x_{i})\\log p(y_{j}|x_{i})\\\\\n",
    "&=& \\color{blue}{- \\sum_{i=1}^{n}\\sum_{j=1}^{m}p(x_{i}, y_{j})\\log p(y_{j}\\mid x_{i})}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\color{blue}{H(Y|X)} &=& - \\sum_{i=1}^{n}\\sum_{j=1}^{m}p(x_{i}, y_{j})\\log p(y_{j}\\mid x_{i}) \\\\\n",
    "&=& - \\left( \\sum_{i=1}^{n}\\sum_{j=1}^{m}p(x_{i}, y_{j})\\log p(x_{i}, y_{j}) - \\sum_{i=1}^{n}\\sum_{j=1}^{m}p(x_{i}, y_{j})\\log p(x_{i}) \\right) \\\\\n",
    "&=& - \\left( \\sum_{i=1}^{n}\\sum_{j=1}^{m}p(x_{i}, y_{j})\\log p(x_{i}, y_{j}) - \\sum_{i=1}^{n}p(x_{i})\\log p(x_{i}) \\right) \\\\\n",
    "&=& H(X, Y) - H(X)\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 交叉熵（Cross Entropy）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉熵主要用于度量两个概率分部间的差异性信息，$p(x)~$对$~q(x)~$的交叉熵表示$~q(x)~$分布的自信息对$~p(x)~$分布的期望:  \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "H(p(x),q(x)) &=& E_{x \\sim p}\\left[ -\\log q(x) \\right] \\\\\n",
    "&=& - \\sum_{i=1}^{n}p(x_{i})\\log q(x_{i})\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "其中，$p(x)$是真实样本分布，$q(x)$是预测得到样本分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression的损失函数是交叉熵，也叫负对数似然，定义为: \n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left( y_{i}\\log h_{\\theta}(x_{i}) + (1-y_{i})\\log (1-h_{\\theta}(x_{i})) \\right)\n",
    "$$\n",
    "\n",
    "其中，$y_{i}$为第$~i~$个样本的真实标签，$h~$是 Sigmoid 输出的预测值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 相对熵（KL Divergence）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相对熵经常也叫KL散度，在贝叶斯推理中，$D_{KL}(p\\|q)$ 衡量当你修改了先验分布$~q~$到后验分布$~p~$的信念之后带来的信息增益。  \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "D_{KL}(p\\|q) &=& E_{x \\sim p}\\left[\\log \\frac{p(x)}{q(x)} \\right] = - E_{x \\sim p}\\left[\\log \\frac{q(x)}{p(x)} \\right] \\\\\n",
    "&=& - \\sum_{i=1}^{n}p(x_{i}) \\log \\frac{q(x_{i})}{p(x_{i})}  \\\\\n",
    "&=& H(p,q) - H(p)\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单对比交叉熵和相对熵，可以发现仅仅差了一个$H(p)$，如果从优化角度来看，$p~$是真实分布，是固定值，最小化KL散度时，$H(p)~$可以省略，此时交叉熵等价于KL散度。  \n",
    "\n",
    "相对熵（KL Divergence）较交叉熵（Cross Entropy）有更多的优异性质: \n",
    "1. 当$~p~$分布和$~q~$分布相等时，KL散度为0，这是一个非常好的性质；\n",
    "\n",
    "2. 可以证明是非负的（Jensen不等式证明）\n",
    "\n",
    "3. <font color=red>非对称的</font>，通过公式可以看出，KL散度是衡量两个分布的不相似性，不相似性越大，则值越大，当完全相同时，取值为0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 互信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "互信息可以评价两个分部之间的距离，主要归因于其对称性。相对熵（KL Divergence）不满足对称性，故通常说相对熵是评价分布的相似程度，而不会说距离。 \n",
    "\n",
    "互信息的定义：<font color=blue>一个随机变量由于已知另一个随机变量而减少的不确定性。</font>  \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "I(X,Y) &=& H(X) - H(Y|X) = H(Y) - H(Y|X) \\\\\n",
    "&=& H(X) + H(Y) - H(X,Y) \\\\\n",
    "&=& H(X,Y) - H(X|Y) - H(Y|X)\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 信息增益"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息增益是决策树ID3算法在进行特征切割时使用的划分准则，其物理意义和互信息完全相同。  \n",
    "\n",
    "$$\n",
    "g(D,A) = H(D) - H(D|A)\n",
    "$$\n",
    "\n",
    "$D$表示数据集，$A$表示特征，信息增益表示得到$A$的信息而使得类$X$的不确定度下降的程度，在ID3中，需选择一个$A$使得信息增益最大，这样可以使得分类系统进行快速决策。  \n",
    "\n",
    "在数值上，信息增益和互信息完全相同，但意义不一样，需要区分。当我们说互信息时候，两个随机变量的地位是相同的，可以认为是纯粹数学工具，不考虑物理意义。当我们说信息增益时，是把一个变量看成是减少另一个变量不确定度的手段。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 信息增益率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以信息增益作为划分数据集的特征，存在偏向于选取取值较多的特征的问题。信息增益比可以对这一问题进行校正。  \n",
    "\n",
    "$$\n",
    "g_{r}(D,A) = \\frac{g(D, A)}{H(A)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. 基尼系数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基尼系数是决策树CART算法引入的划分特征准则，假设有$~K~$个类，样本点属于第$~k~$类的概率为$~p_{k}$，则概率分布的基尼指数定义为\n",
    "\n",
    "$$\n",
    "\\text{Gini}(p) = \\sum_{k=1}^{K}p_{k}(1-p_{k})=1 - \\sum_{k=1}^{K}p_{k}^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
