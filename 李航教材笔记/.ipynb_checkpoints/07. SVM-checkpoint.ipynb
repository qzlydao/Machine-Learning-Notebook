{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0. 前言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 间隔最大，使其有别于感知机；\n",
    "\n",
    "\n",
    "- 学习算法是求解凸二次规划的最优化算法。\n",
    "\n",
    "\n",
    "分类：\n",
    "\n",
    "| 数据特征分布 | SVM技巧 |\n",
    "| :--: | :--: |\n",
    "| 数据线性可分 | 硬间隔最大化 |\n",
    "| 数据近似线性可分 | 软间隔最大化 |\n",
    "| 数据线性不可分 | 核技巧+软间隔最大化，学习非线性SVM |\n",
    "\n",
    "\n",
    "**核函数:** <font color=red>通过使用核函数学习非线性SVM，等价于隐式地在高维的特征空间中学习线性SVM。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1. 线性可分SVM与硬间隔最大化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.1. 线性可分SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设给定一个特征空间上的训练数据集：\n",
    "\n",
    "$$\n",
    "T = {(x_{1}, y_{1}), (x_{2}, y_{2}),\\cdots (x_{N}, y_{N})}\n",
    "$$\n",
    "\n",
    "其中，$x_{i} \\in \\mathcal{X} = \\mathbf{R}^{n},~ y_{i} \\in \\{+1, -1\\}$\n",
    "\n",
    "- 感知机利用误分类最小的策略，求得分离超平面，不过这时的解有无穷多个；\n",
    "\n",
    "- 线性可分SVM利用间隔最大化求最优分离超平面，这时的解时唯一的。\n",
    "\n",
    "\n",
    "**定义7.1 线性可分SVM:**\n",
    "\n",
    "给定线性可分训练数据集，通过间隔最大或等价地求解相应的凸二次规划问题学习得到的分离超平面为：\n",
    "\n",
    "$$\n",
    "w^{*}\\cdot x + b^{*} = 0\n",
    "$$\n",
    "\n",
    "以及相应的分类决策函数\n",
    "\n",
    "$$\n",
    "f(x) = \\text{sign}(w^{*}\\cdot x + b^{*})\n",
    "$$\n",
    "\n",
    "称为线性可分SVM。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.2 函数间隔和几何间隔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义7.2 函数间隔\n",
    "\n",
    "对于给定的训练数据集$T$ 和超平面 $(w,b)$，定义超平面$(w,b)$关于样本点$(x_{i}, y_{i})$的函数间隔为 \n",
    "\n",
    "$$\n",
    "\\hat{\\gamma}_{i} = y_{i}(w\\cdot x_{i} + b) \\tag{7.3}\n",
    "$$\n",
    "\n",
    "定义超平面$(w,b)$关于训练数据集$T$的函数间隔为超平面$(w,b)$关于$T$中所有样本点$(x_{i}, y_{i})$的函数间隔之最小值，即\n",
    "\n",
    "$$\n",
    "\\hat{\\gamma} = \\min_{i=1,\\cdots,N} \\hat{\\gamma}_{i} \\tag{7.4}\n",
    "$$\n",
    "\n",
    "#### 定义7.3 几何间隔\n",
    "\n",
    "对于给定的训练数据集$T$ 和超平面 $(w,b)$，定义超平面$(w,b)$关于样本点$(x_{i}, y_{i})$的几何间隔为 \n",
    "\n",
    "$$\n",
    "\\gamma_{i} = y_{i}(\\frac{w}{\\|w\\|}\\cdot x_{i} + \\frac{b}{\\|w\\|}) \\tag{7.5}\n",
    "$$\n",
    "\n",
    "定义超平面$(w,b)$关于训练数据集$T$的函数间隔为超平面$(w,b)$关于$T$中所有样本点$(x_{i}, y_{i})$的函数间隔之最小值，即\n",
    "\n",
    "$$\n",
    "\\gamma = \\min_{i=1,\\cdots,N} \\gamma_{i} \\tag{7.6}\n",
    "$$\n",
    "\n",
    "函数间隔与几何间隔的关系\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\gamma_{i} = \\frac{\\hat{\\gamma}_{i}}{\\|w\\|} \\tag{7.7} \\\\\n",
    "\\gamma = \\frac{\\hat{\\gamma}}{\\|w\\|} \\tag{7.8}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.3 间隔最大化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最大间隔分离超平面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最大间隔分离超平面的问题可以表示为下面的约束优化问题：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\max_{w,b} &\\quad \\gamma \\tag{7.9} \\\\\n",
    "\\text{s.t.} &\\quad y_{i}(\\frac{w}{\\|w\\|}\\cdot x_{i} + \\frac{b}{\\|w\\|}) \\geq \\gamma, \\quad i=1,2,\\cdots,N \\tag{7.10}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "等价于\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\max_{w,b} &\\quad \\frac{\\hat{\\gamma}}{\\|w\\|} \\tag{7.11} \\\\\n",
    "\\text{s.t.} &\\quad y_{i}(w\\cdot x_{i} + b) \\geq \\hat{\\gamma}, \\quad i=1,2,\\cdots,N \\tag{7.12}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "令$\\hat{\\gamma}=1$，得到如下的约束优化问题\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\color{blue}{\\min_{w,b}} &\\quad \\color{blue}{ \\frac{1}{2}\\|w\\|^{2} } \\tag{7.13} \\\\\n",
    "\\color{blue}{\\text{s.t.}} &\\quad \\color{blue}{y_{i}(w\\cdot x_{i} + b) - 1 \\geq 0, \\quad i=1,2,\\cdots,N } \\tag{7.14}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 支持向量和间隔边界"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='07. 支持向量.jpg' style='zoom:50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.4 学习的对偶算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.4.0 拉格朗日对偶复习\n",
    "\n",
    "#### 原始问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑约束优化问题 \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{x \\in \\mathbf{R}^{n}} &\\quad f(x)  \\tag{C.1}\\\\\n",
    "\\text { s.t. } &\\quad c_{i}(x) \\leqslant 0, \\quad i=1,2, \\cdots, k \\tag{C.2}\\\\\n",
    "&\\quad h_{j}(x)=0, \\quad j=1,2, \\cdots, l \\tag{C.3}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "引入拉格朗日函数，考虑$x$的函数：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta_{P}(x) &= \\max _{\\alpha, \\beta: \\alpha_{i} \\geqslant 0} L(x, \\alpha, \\beta)  \\\\\n",
    "&= \\max _{\\alpha, \\beta: \\alpha_{i} \\geqslant 0} \\left[f(x)+\\sum_{i=1}^{k} \\alpha_{i} c_{i}(x)+\\sum_{j=1}^{l} \\beta_{j} h_{j}(x)\\right] \\tag{C.5}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "这里，下标$P$表示原始问题，推导可得到拉格朗日函数的极小极大问题与原问题等价： \n",
    "\n",
    "$$\n",
    "\\min _{x} \\theta_{P}(x)=\\min _{x} \\max _{\\alpha, \\beta: \\alpha_{i} \\geqslant 0} L(x, \\alpha, \\beta) \\tag{C.8}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对偶问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义 \n",
    "\n",
    "$$\n",
    "\\theta_{D}(\\alpha, \\beta)=\\min _{x} L(x, \\alpha, \\beta) \\tag{C.10}\n",
    "$$\n",
    "\n",
    "则原始问题的对偶问题表示为\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\max _{\\alpha, \\beta} \\theta_{D}(\\alpha, \\beta)=\\max _{\\alpha, \\beta} \\min _{x} L(x, \\alpha, \\beta) \\tag{C.12}\\\\\n",
    "\\text { s.t. } \\quad \\alpha_{i} \\geqslant 0, \\quad i=1,2, \\cdots, k \\tag{C.13}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "定义对偶问题的最优值\n",
    "\n",
    "$$\n",
    "d^{*}=\\max _{\\alpha, \\beta: \\alpha_{i} \\geqslant 0} \\theta_{D}(\\alpha, \\beta) \\tag{C.14}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 原始问题与对偶问题的关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**定理1:** 若原始问题和对偶问题都有最优值，则\n",
    "\n",
    "$$\n",
    "d^{*}=\\color{red}{\\max _{\\alpha, \\beta: \\alpha_{i} \\geqslant 0} \\min _{x}} L(x, \\alpha, \\beta) \\leqslant \\color{blue}{\\min _{x} \\max _{\\alpha, \\beta: \\alpha_{i} \\geqslant 0}} L(x, \\alpha, \\beta)=p^{*} \\tag{C.15}\n",
    "$$\n",
    "\n",
    "**定理2:** Slater's condition(充分非必要条件)\n",
    "\n",
    "假设原始问题中的函数<font color=red>$f(x)$和$c_{i}(x)$是凸函数</font>，$h_{j}(x)$是仿射函数；并且假设不等于约束$c_{i}(x)$是严格可行的，<font color=red>即存在$x$，对所有$i$都有$c_{i}(x)<0$</font>，则存在$x^{*},\\alpha^{*},\\beta^{*}$，使$x^{*}$是原始问题的解，$\\alpha^{*},\\beta^{*}$是对偶问题的解，并且\n",
    "\n",
    "$$\n",
    "p^{*}=d^{*}=L\\left(x^{*}, \\alpha^{*}, \\beta^{*}\\right) \\tag{C.20}\n",
    "$$\n",
    "\n",
    "**定理3:** KKT condition(充要条件)\n",
    "\n",
    "假设原始问题中的函数<font color=red>$f(x)$和$c_{i}(x)$是凸函数</font>，$h_{j}(x)$是仿射函数；并且假设不等于约束$c_{i}(x)$是严格可行的，<font color=red>即存在$x$，对所有$i$都有$c_{i}(x)<0$</font>，则$x^{*}$和$\\alpha^{*},\\beta^{*}$分别是原始问题和对偶问题的解的充要条件是$x^{*},\\alpha^{*},\\beta^{*}$满足下面的KKT条件\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\nabla_{x} L\\left(x^{*}, \\alpha^{*}, \\beta^{*}\\right)=0 \\\\\n",
    "\\alpha_{i}^{*} c_{i}\\left(x^{*}\\right)=0, \\quad i=1,2, \\cdots, k \\\\\n",
    "\\alpha_{i}^{*} \\geqslant 0, \\quad i=1,2, \\cdots, k \\\\\n",
    "c_{i}\\left(x^{*}\\right) \\leqslant 0, \\quad i=1,2, \\cdots, k \\\\\n",
    "h_{j}\\left(x^{*}\\right)=0 \\quad j=1,2, \\cdots, l\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "对应为\n",
    "\n",
    "- 梯度条件\n",
    "- 互补松弛条件\n",
    "- 对偶约束条件\n",
    "- 原问题条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.4.1 $w^{*}, b^{*}$的对偶求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对偶算法的好处：\n",
    "\n",
    "1. <font color=red>没了不等式约束条件，更易求解；</font>\n",
    "\n",
    "2. <font color=red>自然引入核函数，进而推广到非线性分类问题</font>\n",
    "\n",
    "原始问题\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\color{blue}{\\min_{w,b}} &\\quad \\color{blue}{ \\frac{1}{2}\\|w\\|^{2} } \\tag{7.13} \\\\\n",
    "\\color{blue}{\\text{s.t.}} &\\quad \\color{blue}{y_{i}(w\\cdot x_{i} + b) - 1 \\geq 0, \\quad i=1,2,\\cdots,N } \\tag{7.14}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "定义拉格朗日函数：\n",
    "$$\n",
    "L(w, b, \\alpha)=\\frac{1}{2}\\|w\\|^{2}-\\sum_{i=1}^{N} \\alpha_{i} y_{i}\\left(w \\cdot x_{i}+b\\right)+\\sum_{i=1}^{N} \\alpha_{i} \\tag{7.18}\n",
    "$$\n",
    "\n",
    "根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：\n",
    "\n",
    "$$\n",
    "\\max _{\\alpha} \\min _{w, b} L(w, b, \\alpha) \n",
    "$$\n",
    "\n",
    "（1） 求$\\min _{w, b} L(w, b, \\alpha)$\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\nabla_{w} L(w, b, \\alpha)=w-\\sum_{i=1}^{N} \\alpha_{i} y_{i} x_{i}=0 \\\\\n",
    "\\nabla_{b} L(w, b, \\alpha)=-\\sum_{i=1}^{N} \\alpha_{i} y_{i}=0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "得\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w = \\sum_{i=1}^{N}\\alpha_{i}y_{i}x_{i} \\tag{7.19} \\\\\n",
    "\\sum_{i=1}^{N}\\alpha_{i}y_{i} = 0 \\tag{7.20}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "将（7.19）（7.20）带入式（7.18），得\n",
    "\n",
    "$$\n",
    "\\min _{w, b} L(w, b, \\alpha)=-\\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(x_{i} \\cdot x_{j}\\right)+\\sum_{i=1}^{N} \\alpha_{i}\n",
    "$$\n",
    "\n",
    "（2）求$\\min_{w,b}L(w,b,\\alpha)$对$\\alpha$的极大\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\max _{\\alpha} & -\\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(x_{i} \\cdot x_{j}\\right)+\\sum_{i=1}^{N} \\alpha_{i} \\tag{7.21} \\\\\n",
    "\\text { s.t. } & \\sum_{i=1}^{N} \\alpha_{i} y_{i}=0 \\\\\n",
    "& \\alpha_{i} \\geqslant 0, \\quad i=1,2, \\cdots, N\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "将（7.21）的目标函数转化为求极小，就得到与之等价的对偶最优化问题：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min _{\\alpha} & \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(x_{i} \\cdot x_{j}\\right)-\\sum_{i=1}^{N} \\alpha_{i} \\tag{7.22} \\\\\n",
    "\\text { s.t. } & \\sum_{i=1}^{N} \\alpha_{i} y_{i}=0 \\tag{7.23}  \\\\\n",
    "& \\alpha_{i} \\geqslant 0, \\quad i=1,2, \\cdots, N \\tag{7.24} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**定义7.4（支持向量）** 考虑原始最优化问题（7.13）~（7.14）及对偶最优化问题（7.22）~（7.24），将训练数据集中对应于$\\alpha_{i}^{*}>0$的样本点$(x_{i},y_{i})$的实例$x_{i}\\in \\mathrm{R}^{n}$称为支持向量。\n",
    "\n",
    "根据这一定义，支持向量一定在间隔边界上。由KKT互补条件可知，\n",
    "\n",
    "$$\n",
    "\\alpha_{i}^{*}\\left(y_{i}\\left(w^{*} \\cdot x_{i}+b^{*}\\right)-1\\right)=0, \\quad i=1,2, \\cdots, N\n",
    "$$\n",
    "\n",
    "对应于$\\alpha_{i}^{*}>0$的实例$x_{i}$，有\n",
    "\n",
    "$$\n",
    "y_{i}\\left(w^{*} \\cdot x_{i}+b^{*}\\right)-1=0\n",
    "$$\n",
    "\n",
    "或\n",
    "\n",
    "$$\n",
    "w^{*} \\cdot x_{i}+b^{*} = \\pm 1\n",
    "$$\n",
    "\n",
    "即$x_{i}$一定在间隔上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2 线性SVM与软间隔最大化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了解决某些样本点是特异点（outlier）的这个问题，可以对每个样本点$(x_{i}, y_{i})$引进一个松弛变量$\\xi_{i} \\geq 0$，使得函数变量加上松弛变量大于等于1。因此，线性不可分的线性SVM的学习问题变成如下凸二次规划问题：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min _{w, b, \\xi} & \\frac{1}{2}\\|w\\|^{2}+C \\sum_{i=1}^{N} \\xi_{i} \\tag{7.32} \\\\\n",
    "\\text { s.t. } & y_{i}\\left(w \\cdot x_{i}+b\\right) \\geqslant 1-\\xi_{i}, \\quad i=1,2, \\cdots, N  \\tag{7.33}\\\\\n",
    "& \\xi_{i} \\geqslant 0, \\quad i=1,2, \\cdots, N \\tag{7.34}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "原始问题的对偶问题是\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min _{\\alpha} & \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(x_{i} \\cdot x_{j}\\right)-\\sum_{i=1}^{N} \\alpha_{i} \\tag{7.37} \\\\\n",
    "\\text { s.t. } & \\sum_{i=1}^{N} \\alpha_{i} y_{i}=0 \\tag{7.38} \\\\\n",
    "& 0 \\leqslant \\alpha_{i} \\leqslant C, \\quad i=1,2, \\cdots, N \\tag{7.39}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**定理7.3** 设$\\alpha^{*}=\\left(\\alpha_{1}^{*}, \\alpha_{2}^{*}, \\cdots, \\alpha_{N}^{*}\\right)^{\\mathrm{T}}$是对偶问题（7.37）~（7.39）的一个解，若存在$\\alpha^{*}$的一个分量$\\alpha_{j}^{*}, 0 < \\alpha_{j}^{*} < C$，则原始问题（7.32）~（7.34）的解$w^{*},b^{*}$可按下式求得：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w^{*}=\\sum_{i=1}^{N} \\alpha_{i}^{*} y_{i} x_{i} \\tag{7.50} \\\\\n",
    "b^{*}=y_{j}-\\sum_{i=1}^{N} y_{i} \\alpha_{i}^{*}\\left(x_{i} \\cdot x_{j}\\right) \\tag{7.51}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### 合页损失函数\n",
    "\n",
    "线性支持向量机原始最优化问题（7.32）~（7.34）等价于最优化问题\n",
    "\n",
    "$$\n",
    "\\min _{w, b} \\sum_{i=1}^{N}\\left[1-y_{i}\\left(w \\cdot x_{i}+b\\right)\\right]_{+}+\\lambda\\|w\\|^{2} \\tag{7.63}\n",
    "$$\n",
    "\n",
    "<img src='07. 合页损失函数.png' style='zoom:50%'/>\n",
    "\n",
    "合页损失函数是0-1损失函数的上界。上图中虚线显示的是感知机的损失函数 $[-y_{i}(w\\cdot x_{i} + b)]_{+}$。这是，当样本点$(x_{i}, y_{i})$被正确分类时，损失是0，否则损失是$-y_{i}(w\\cdot x_{i} + b)$。相比之下，合页损失函数不仅要分类正确，而且确信度足够高时损失才是0。也就是说，合页损失函数对学习有更高的要求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.3 非线性SVM与核函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3.0 希尔伯特空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hilbert Space: `完备的，可能是无限维的，被赋予内积的 线性空间。`\n",
    "\n",
    "1. 完备的：对极限是封闭的\n",
    "\n",
    "$$\n",
    "\\{k_{n}\\} \\\\\n",
    "\\lim_{n\\rightarrow \\infty} \\ k_{n} = k \\in \\mathcal{H}\n",
    "$$\n",
    "\n",
    "2. 内积性质\n",
    "\n",
    "    - 对称性\n",
    "    - 正定性\n",
    "    - 线性\n",
    "    \n",
    "    设 $f,g \\in \\mathcal{H}$，则有\n",
    "    \n",
    "    $$\n",
    "    <f, g> \\ = \\ <g, f> \\\\\n",
    "    <f, f>\\ \\geq \\ 0, '=' \\Leftrightarrow \\ f=0\\\\\n",
    "    <r_{1}f_{1} + r_{2}f_{2}, g> \\ =  \\ r_{1}<f_{1},g> \\ + \\  r_{2}<f_{2}, g>\n",
    "    $$\n",
    "    \n",
    "3. 线性空间\n",
    "    \n",
    "    向量空间，满足加法和数乘定律。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3.1 核技巧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='07. 非线性分类与核技巧.png' style='zoom:50%'/>\n",
    "\n",
    "Kernel Method: 非线性问题往往不好求解，说采取的方法是进行非线性变换，将非线性问题变换为线性问题，通过解变换后的线性问题的方法求解原来的非线性问题。\n",
    "\n",
    "**定义7.6（核函数）** 设$\\mathcal{X}$是输入空间，又设$\\mathcal{H}$为特征空间（希尔伯特空间），如果存在一个从$\\mathcal{X}$到$\\mathcal{H}$的映射\n",
    "\n",
    "$$\n",
    "\\phi(x): \\mathcal{X} \\rightarrow \\mathcal{H} \\tag{7.65}\n",
    "$$\n",
    "\n",
    "使得对所有 $x,z \\in \\mathcal{X}$，函数$K(x,z)$满足条件\n",
    "\n",
    "$$\n",
    "K(x,z) = \\phi(x) \\cdot \\phi(z) \\tag{7.66}\n",
    "$$\n",
    "\n",
    "则称$K(x,z)$为核函数，$\\phi(x)$为映射函数。\n",
    "\n",
    "<font color=red>核技巧（Kernel Trick）的思想：</font> <font color=blue>在学习与预测中只定义核函数$K(x,z)$，而不显示地定义映射函数$\\phi$。通常，直接计算核函数$K(x,z)$比较容易，而通过$\\phi(x)$和$\\phi(z)$计算$K(x,z)$并不容易。</font>\n",
    "\n",
    "#### 核技巧在SVM中应用\n",
    "\n",
    "在线性支持向量机的对偶问题中，只需要将目标函数和决策函数中的内积$x_{i}\\cdot x_{j}$用核函数$K\\left(x_{i}, x_{j}\\right)=\\phi\\left(x_{i}\\right) \\cdot \\phi\\left(x_{j}\\right)$来代替。此时的对偶问题的目标函数称为\n",
    "\n",
    "$$\n",
    "W(\\alpha)=\\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j} K\\left(x_{i}, x_{j}\\right)-\\sum_{i=1}^{N} \\alpha_{i} \\tag{7.67}\n",
    "$$\n",
    "\n",
    "分类决策函数中的内积也可以用核函数代替\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(x) &=\\operatorname{sign}\\left(\\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i} \\phi\\left(x_{i}\\right) \\cdot \\phi(x)+b^{*}\\right) \\\\\n",
    "&=\\operatorname{sign}\\left(\\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i} K\\left(x_{i}, x\\right)+b^{*}\\right)\n",
    "\\end{aligned} \\tag{7.68}\n",
    "$$\n",
    "\n",
    "#### 正定核的充要条件\n",
    "\n",
    "设 $K: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbf{R}$ 是对称函数，则$K(x,z)$为正定核函数的充要条件是对任意$x_{i} \\in \\mathcal{X}, i=1,2, \\cdots, m, K(x, z)$对应的Gram矩阵：\n",
    "\n",
    "$$\n",
    "K=\\left[K\\left(x_{i}, x_{j}\\right)\\right]_{m \\times m} \\tag{7.85}\n",
    "$$\n",
    "\n",
    "是半正定矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3.3 常用核函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 多项式核函数\n",
    "\n",
    "$$\n",
    "K(x, z)=(x \\cdot z+1)^{p} \\tag{7.88}\n",
    "$$\n",
    "\n",
    "    对应的决策函数\n",
    "\n",
    "$$\n",
    "f(x)=\\operatorname{sign}\\left(\\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i}\\left(x_{i} \\cdot x+1\\right)^{p}+b^{*}\\right) \\tag{7.89}\n",
    "$$\n",
    "\n",
    "2. 高斯核函数\n",
    "\n",
    "$$\n",
    "K(x, z)=\\exp \\left(-\\frac{\\|x-z\\|^{2}}{2 \\sigma^{2}}\\right) \\tag{7.90}\n",
    "$$\n",
    "\n",
    "    对应分类决策函数\n",
    "    \n",
    "$$\n",
    "f(x)=\\operatorname{sign}\\left(\\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i} \\exp \\left(-\\frac{\\left\\|x-x_{i}\\right\\|^{2}}{2 \\sigma^{2}}\\right)+b^{*}\\right) \\tag{7.91}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
