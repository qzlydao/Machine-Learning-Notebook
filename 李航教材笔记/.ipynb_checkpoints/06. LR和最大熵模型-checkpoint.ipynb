{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第6章 Logistic Regression和最大熵模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 前言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 虽然模型名称中有Regression，但它是判别模型；  \n",
    "        \n",
    "        \n",
    "2. Logistic Regression是最大熵模型的特例；  \n",
    "  \n",
    "  \n",
    "3. 最大熵模型的学习过程可以形式化为约束最优化问题，可以用拉格朗日对偶性求解：\n",
    "  - 原问题：$\\min_{P}\\max_{w}L(P,w)$\n",
    "  - 对偶问题： $\\max_{w}\\min_{P}L(P,w)$  \n",
    "  \n",
    "    对偶问题中的$\\min_{P}L(P,w)$是容易求解的，因此<font color=blue>归结为对偶函数$\\Psi(w)$的极大化问题。</font>\n",
    "\n",
    "\n",
    "4. 可以证明<font color=blue>对偶函数的极大化问题等价于最大熵模型的MLE</font>.  \n",
    "\n",
    "\n",
    "5. 对于似然函数的最优化问题，介绍了IIS和拟牛顿法求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 Logistic Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(x)=P(X \\leq x) = \\frac{1}{1+e^{-(x-u)/\\gamma}} \\\\\n",
    "f(x) = F^{\\prime}(x) = \\frac{e^{-(x-u)/\\gamma}}{\\gamma(1+e^{-(x-u)/\\gamma})^{2}}\n",
    "$$\n",
    "$u$ 位置参数  \n",
    "$\\gamma$ 形状参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特别地，当$u=0,\\gamma=1$时:\n",
    "$$\n",
    "F(x) = \\frac{1}{1+e^{-x}} = \\frac{e^{x}}{1+e^{x}}  \\\\\n",
    "f(x) = \\frac{e^{-x}}{(1+e^{-x})^{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='6.1_逻辑斯蒂分布.png' style='zoom:50%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 二项Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二项Logistic Regression模型是如下的条件概率分布:  \n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "P(Y=1|x) = \\frac{\\exp (w \\cdot x +b)}{1 + \\exp (w \\cdot x +b)}  \\tag{6.3} \\\\\n",
    "P(Y=0|x) = \\frac{1}{1 + \\exp (w \\cdot x +b)} \\tag{6.4}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "这里，$x \\in \\mathbf{R}^{n}$是输入，$Y \\in \\{0,1\\}$是输出。 \n",
    "  \n",
    "  \n",
    "对给定输入$x$，按式$(6.3)$和式(6.4)求得$P(Y=1|x)$和$P(Y=0|x)$.比较两个条件概率，将$x$分到概率较大的那一类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若记$w=(w^{(1)},\\cdots,w^{(n)},b)^{T}$，$x=(x^{(1)},\\cdots,x^{(n)},1)^{T}$，则Logistic Regression如下:  \n",
    "  \n",
    "  \n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "P(Y=1|x) = \\frac{\\exp (w \\cdot x)}{1 + \\exp (w \\cdot x)}  \\tag{6.5} \\\\\n",
    "P(Y=0|x) = \\frac{1}{1 + \\exp (w \\cdot x)} \\tag{6.6}\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对数几率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{logit}(p)=\\log \\frac{p}{1-p}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于LR模型来说:\n",
    "$$\n",
    "\\log \\frac{P(Y=1|x)}{1-P(Y=1|x)} = w\\cdot x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 模型参数估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定数据集$T=\\{(x_{1},y_{1}), (x_{1},y_{1}),\\cdots,  (x_{N},y_{N})\\}$，$x_{i} \\in \\mathbf{R}^{n}$，$y_{i} \\in \\{0,1\\}$. \n",
    "  \n",
    "  \n",
    "设： $P(Y=1|x)=\\pi(x)$，$P(Y=0|x)=1-\\pi(x)$，则似然函数可写成:\n",
    "$$\n",
    "\\prod_{i=1}^{N}[\\pi(x_{i})]^{y_{i}}[1-\\pi(x_{i})]^{1-y_{i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数似然函数为\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "L(w) &=& \\sum_{i=1}^{N}[y_{i}\\log \\pi(x_{i})+(1-y_{i})\\log (1-\\pi(x_{i}))] \\\\\n",
    "&=& \\sum_{i=1}^{N}\\left[y_{i}\\log \\frac{\\pi(x_{i})}{1-\\pi(x_{i}} + \\log (1-\\pi(x_{i}))\\right] \\\\\n",
    "&=& \\sum_{i=1}^{N}\\left[y_{i}(w\\cdot x) - \\log (1+\\exp(w \\cdot x_{i}))\\right] \n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用牛顿法或拟牛顿法求出$w$的估计值$\\hat{w}$，于是得到LR模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4 多项LR回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设$y_{i} \\in \\{1,2,\\cdots, K\\}$，则多项LR模型是:\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "P(Y=k|x) &=& \\frac{\\exp (w_{k} \\cdot x)}{1 + \\sum_{k=1}^{K-1}\\exp (w_{k} \\cdot x)} ,k=1,2,\\cdots,K-1 \\\\\n",
    "P(Y=K|x) &=& \\frac{1}{1 + \\sum_{k=1}^{K-1}\\exp (w_{k} \\cdot x)}\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采用的是“one VS all”思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 最大熵模型（Maximum Entropy Model）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.0 信息、熵相关补充知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自信息\n",
    "自信息表示某一事件发生时所带来的信息量的多少:\n",
    "$$\n",
    "I(p_{i}) = -\\log(p_{i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 信息熵\n",
    "上述自信息描述的是随机变量的某个事件发生所带来的信息量，而信息熵通常用来描述整个随机分布所带来的信息量平均值，更具统计特性。信息熵也叫香农熵，在机器学习中，由于熵的计算是依据样本数据而来，故也叫经验熵。其公式定义如下：  \n",
    "  \n",
    "  \n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "H(X) &=& E_{x \\sim p}\\left[ I(x) \\right] \\\\\n",
    "&=& -E_{x \\sim p} \\left[ \\log p(x) \\right] \\\\\n",
    "&=& -\\sum_{i=1}^{n}p(x_{i})\\log p(x_{i}) \\\\\n",
    "&=& - \\int_{x}p(x)\\log p(x)dx\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 条件熵\n",
    "条件熵的定义为：在X给定条件下，Y的条件概率分布的熵对X的数学期望。\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "H(Y|X) &=& E_{x \\sim p} \\left[ H(Y|X=x) \\right] = \\sum_{i=1}^{n}p(x)H(Y|X=x) \\\\\n",
    "&=& -\\sum_{i=1}^{n}p(x_{i}) \\sum_{j=1}^{m}p(y_{j}|x_{i})\\log p(y_{j}|x_{i}) \\\\\n",
    "&=& -\\sum_{i=1}^{n}\\sum_{j=1}^{m}p(x_{i},y_{j})\\log p(y_{j}|x_{i})\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 最大熵原理\n",
    "最大熵原理认为：<font color=blue>在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设随机变量$X$的概率分布是$P(X)$，则其熵是\n",
    "$$\n",
    "H(P) = -\\sum_{x}P(x)\\log P(x)\n",
    "$$\n",
    "满足如下不等式:\n",
    "$$\n",
    "0 \\leq H(P) \\leq \\log |X|\n",
    "$$\n",
    "$|X|$是$X$的取值个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 最大熵模型的定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 问题描述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设分类模型是一个条件概率分布$P(Y|X)$，表示对于给定的输入$X$，以条件概率输出$Y$.  \n",
    "  \n",
    "  \n",
    "数据集:  $T=\\{(x_{1},y_{1}), (x_{1},y_{1}),\\cdots,  (x_{N},y_{N})\\}$  \n",
    "  \n",
    "  \n",
    "学习的目标是: <font color=blue>用最大熵原理，选择最好的分类模型。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 经验分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于给定的数据，可以确定$P(X,Y)$和$P(X)$的经验分布，用$\\tilde{P}(X,Y)$和$\\tilde{P}(X)$表示:  \n",
    "  \n",
    "  \n",
    "$$\n",
    "\\tilde{P}(X=x, Y=y) = \\frac{\\nu(X=x, Y=y)}{N}  \\\\\n",
    "\\tilde{P}(X=x) = \\frac{\\nu(X=x)}{N} \n",
    "$$  \n",
    "  \n",
    "  \n",
    "$\\nu(X=x, Y=y)$ 表示训练数据中样本$(x,y)$出现的频数；  \n",
    "$\\nu(X=x)$ 表示训练数据中输入$x$出现的频数；  \n",
    "$N$ 训练样本容量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征函数及期望"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用特征函数$f(x,y)$描述输入$x$和输出$y$之间的某一事实，定义为（<font color=red>特征函数可以是任一实值函数</font>） \n",
    "\n",
    "$$\n",
    "f(x,y) = \\left\\{\\begin{matrix}\n",
    " 1 & x,y满足某一事实\\\\\n",
    " 0 & other\n",
    "\\end{matrix}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征函数$f(x,y)$关于经验分布$\\tilde{P}(X,Y)$的期望值:\n",
    "$$\n",
    "E_{\\tilde{P}}(f) = \\sum_{x,y}\\tilde{P}(x,y) f(x,y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征函数$f(x,y)$关于<font color=blue>模型$P(Y|X)$</font>与经验分布$\\tilde{P}(X)$的期望值:\n",
    "$$\n",
    "E_{P}(f) = \\sum_{x,y}\\tilde{P}(x)P(y|x) f(x,y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果模型能够获取训练数据中的信息，那么可以假设则两个期望相等:  \n",
    "  \n",
    "  \n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "E_{\\tilde{P}}(f) &=& E_{P}(f) \\tag{6.10} \\\\\n",
    "\\sum_{x,y}\\tilde{P}(x,y) f(x,y) &=& \\sum_{x,y}\\tilde{P}(x)P(y|x) f(x,y) \\tag{6.11}\n",
    "\\end{eqnarray}\n",
    "$$  \n",
    "\n",
    "\n",
    "我们将上面的等式作为模型的约束条件。假如有$n$个特征函数，那么就有$n$个约束条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=red>最大熵模型</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设满足所有约束条件的模型集合为:\n",
    "$$\n",
    "\\mathcal{C} \\equiv \\left\\{ P \\in \\mathcal{P} |E_{\\tilde{P}}(f_{i}) = E_{P}(f_{i}) ,i=1,\\dots,n\\right\\} \\tag{6.12}\n",
    "$$\n",
    "定义在<font color=blue>条件概率分布$P(Y|X)$上的条件熵</font>为:\n",
    "$$\n",
    "\\color{red}{H(P) = -\\sum_{x,y}\\tilde{P}(x)P(y|x)\\log P(y|x)} \\tag{6.13}\n",
    "$$\n",
    "则模型集合$\\mathcal{C}$中条件熵$H(P)$最大的模型称为最大熵模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 最大熵模型的学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最大熵模型的学习可以<font color=red>形式化为约束最优化问题。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left\\{\\begin{matrix}\n",
    " \\max_{P\\in \\mathcal{C}} & -\\sum_{x,y}\\tilde{P}(x)P(y|x)\\log P(y|x) \\\\\n",
    " s.t & E_{\\tilde{P}}(f_{i}) = E_{P}(f_{i}) ,i=1,\\dots,n \\\\\n",
    "  & \\sum_{y}P(y|x) = 1\n",
    "\\end{matrix}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照最优化的习惯，将最大化问题改写为等价的最小值问题:  \n",
    "  \n",
    "  \n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\color{blue}{\\min_{P\\in \\mathcal{C}}\\quad -H(P) = \\sum_{x,y}\\tilde{P}(x)P(y|x)\\log P(y|x)} \\tag{6.14} \\\\\n",
    " \\color{blue}{\\text{s.t} \\quad \\quad  E_{P}(f_{i}) - E_{\\tilde{P}}(f_{i}) = 0 ,i=1,\\dots,n} \\tag{6.15}  \\\\\n",
    "\\quad \\color{blue}{\\sum_{y}P(y|x) = 1}  \\tag{6.16} \n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入拉格朗日乘子$w_{0},w_{1},\\cdots, w_{n}$，定义拉格朗日函数$L(P,w)$，将约束优化的原始问题转换为无约束最优化的对偶问题，通过求解对偶问题求解元素问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\color{red}{L(P, w)} \\equiv &-H(P)+w_{0}\\left(1-\\sum_{y} P(y \\mid x)\\right)+\\sum_{i=1}^{n} w_{i}\\left(E_{\\tilde{P}}\\left(f_{i}\\right)-E_{P}\\left(f_{i}\\right)\\right) \\\\\n",
    "=& \\sum_{x, y} \\tilde{P}(x) P(y \\mid x) \\log P(y \\mid x)+w_{0} \\underset{\\color{red}{=0}}{\\left(1-\\sum_{y} P(y \\mid x)\\right)}+\\\\\n",
    "& \\sum_{i=1}^{n} w_{i}\\left(\\sum_{x, y} \\tilde{P}(x, y) f_{i}(x, y)-\\sum_{x, y} \\tilde{P}(x) P(y \\mid x) f_{i}(x, y)\\right)\n",
    "\\end{aligned} \\tag{6.17}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最优化的原始问题:\n",
    "$$\n",
    "\\min_{P\\in \\mathcal{C}} \\max_{w} L(P,w) \\tag{6.18}\n",
    "$$\n",
    "对应的对偶问题:\n",
    "$$\n",
    "\\max_{w} \\underset{\\Psi(w)}{\\underbrace{\\min_{P\\in \\mathcal{C}} L(P,w)} }  \\tag{6.19}\n",
    "$$  \n",
    "  \n",
    "  \n",
    "<font color=red>由于$L(p,w)$是$P$的凸函数，所以原始问题与对偶问题的解等价。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1 求解$\\min_{P\\in \\mathcal{C}} L(P,w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义对偶函数\n",
    "$$\n",
    "\\Psi(w) = \\min_{P\\in \\mathcal{C}} L(P,w) = L(P_{w},w) \\tag{6.20}\n",
    "$$\n",
    "将其解记作:\n",
    "$$\n",
    "P_{w} = \\arg \\min_{P\\in \\mathcal{C}} L(P,w) = P_{w}(y|x) \\tag{6.21}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求$L(P,w)$对$P(y|x)$的偏导数:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L(P, w)}{\\partial P(y \\mid x)} &=\\sum_{x, y} \\tilde{P}(x)\\left(\\log P(y \\mid x)+1\\right)-\\sum_{y} w_{0}-\\sum_{x, y}\\left(\\tilde{P}(x) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right) \\\\\n",
    "&=\\sum_{x, y} \\underset{\\color{red}{\\neq 0}}{\\tilde{P}(x)} \\underset{ \\color{red}{= 0}}{\\left(\\log P(y \\mid x)+1-w_{0}-\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)} = 0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解得  \n",
    "  \n",
    "  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(y|x) &= \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y) + w_{0} - 1 \\right) \\\\\n",
    "&= \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)  \\right) \\exp \\left( w_{0} - 1 \\right) \\\\\n",
    "&= \\frac{\\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)  \\right) }{\\exp \\left( 1- w_{0} \\right) }\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于$\\sum_{y}P(y|x)=1$，得:  \n",
    "  \n",
    "  \n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\color{blue}{P_{w}(y|x)} &=& \\color{blue}{\\frac{1}{Z_{w}(x)} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y) \\right) }  \\tag{6.22}\\\\\n",
    "\\color{blue}{Z_{w}(x)} &=& \\color{blue}{\\sum_{y}\\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)  \\right) } \\tag{6.23}\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>由（6.22）和（6.23）表示的模型就是最大熵模型。</font>可以看到，LR是最大熵模型的一个特列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2 求解对偶问题外部的极大化问题$\\max_{w} \\Psi (w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "记其解为$w^{*}$,即\n",
    "$$\n",
    "w^{*} = \\arg \\max_{w} \\Psi(w) \\tag{6.25}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用得到$w^{*}$来表示$P^{*} \\in \\mathcal{C}$，这里$P^{*} = P_{w^{*}} = P_{w^{*}}(y|x) $就是学到的最优模型（最大熵模型）。也就是说，<font color=red>最大熵模型的学习归结为对偶函数$\\Psi(w)$的极大化问题。</font>该问题在6.3节进行讲解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.4 极大似然估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 补充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设随机变量为$X$，样本集$T=(x_{1},x_{x},\\cdots,x_{n})^{T}$,联合概率的似然函数:\n",
    "$$\n",
    "L(x_{1},x_{x},\\cdots,x_{n};\\theta) = \\prod_{i=1}^{n}p(x_{i};\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设$X$的取值有$k$个，用$v_{1},v_{2}, \\cdots v_{k}$表示，用$\\nu(X=v_{i})$表示样本$v_{i}$出现的频数，则似然函数可写成:\n",
    "$$\n",
    "L(x_{1},x_{x},\\cdots,x_{n};\\theta) = \\prod_{i=1}^{k}p(X=v_{i};\\theta)^{\\nu(X=v_{i})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "等式两边同时开$n$次方:\n",
    "$$\n",
    "L(x_{1},x_{x},\\cdots,x_{n};\\theta)^{\\frac{1}{n}} = \\prod_{i=1}^{k}p(X=v_{i};\\theta)^{\\frac{\\nu(X=v_{i})}{n}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机变量$X$的经验分布为$\\tilde{p}(x) = \\frac{\\nu(X=v_{i})}{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "又极大化$L(x_{1},x_{x},\\cdots,x_{n};\\theta)^{\\frac{1}{n}}$和$L(x_{1},x_{x},\\cdots,x_{n};\\theta)$结果一致，因此似然函数可写成:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(x_{1},x_{x},\\cdots,x_{n};\\theta) = \\prod_{x}p(x;\\theta)^{\\tilde{p}(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最大熵模型的极大似然函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已知训练数据的经验分布为$\\tilde{P}(X,Y)$，条件分布的对数似然函数表示为\n",
    "$$\n",
    "L_{\\tilde{P}}(P_{w}) = \\log \\prod_{x,y}P(y|x)^{\\tilde{P}(x,y)} = \\sum_{x,y}\\tilde{P}(x,y) \\log P(y|x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将(6.22)带入上式\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\color{red}{L_{\\tilde{P}}(P_{w})} &=& \\sum_{x,y}\\tilde{P}(x,y) \\log P(y|x) \\\\\n",
    "&=& \\sum_{x,y}\\tilde{P}(x,y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)  - \\sum_{x,y}\\tilde{P}(x,y) \\log Z_{w}(x) \\\\\n",
    "&=& \\color{red}{\\sum_{x,y}\\left\\{\\tilde{P}(x,y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y) \\right\\} - \\sum_{x}\\tilde{P}(x) \\log Z_{w}(x)}\n",
    "\\end{eqnarray} \\tag{6.26}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对偶函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将式(6.22)带入Lagrange函数(6.17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\Psi(w)=& \\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x) \\log P_{w}(y \\mid x)+\\\\\n",
    "& \\sum_{i=1}^{n} w_{i}\\left(\\sum_{x, y} \\tilde{P}(x, y) f_{i}(x, y)-\\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x) f_{i}(x, y)\\right) \\\\\n",
    "=& \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)+\\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x) \\underline{\\left(\\log P_{w}(y \\mid x)-\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)} \\\\\n",
    "=& \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x) \\log Z_{w}(x) \\\\\n",
    "=& \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x} \\tilde{P}(x) \\log Z_{w}(x)\n",
    "\\end{aligned} \\tag{6.27}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上式中划线部分:\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\log P_{w}(y \\mid x)-\\sum_{i=1}^{n} w_{i} f_{i}(x, y) &=& \\log P_{w}(y \\mid x)-\\log \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right) \\\\\n",
    "&=& - \\log \\frac{\\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)}{P_{w}(y \\mid x)} \\\\\n",
    "&=& - \\log Z_{w}(x)\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一步用到了$\\sum_{y}P(y|x)=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较式(6.27)和式(6.27),可得\n",
    "$$\n",
    "\\Psi(w)=L_{\\tilde{P}}(P_{w}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既然对偶函数等价于对数似然函数，于是证明了<font color=blue>最大熵模型学习中的对偶函数极大化等价于最大熵模型的极大似然估计这一事实。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
