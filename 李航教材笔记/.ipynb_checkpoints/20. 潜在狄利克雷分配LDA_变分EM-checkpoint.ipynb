{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20.4 LDA的变分EM算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA的变分EM算法具有推理与学习效率高的优点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.4.0 KL散度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL散度（KL divergence，Kullback-Leibler divergence）是描述<font color=blue>两个概率分布$~Q(x)~$和$~P(x)~$相似度的一种度量</font>，记作$D(Q\\| P)$。\n",
    "\n",
    "对于离散随机变量，KL散度定义为:  \n",
    "\n",
    "$$\n",
    "D(Q\\|P) = \\sum_{i}Q(i)\\log \\frac{Q(i)}{P(i)} \n",
    "$$\n",
    "\n",
    "对于连续随机变量，KL散度定义为:\n",
    "\n",
    "$$\n",
    "D(Q\\|P) = \\int Q(x)\\log \\frac{Q(x)}{P(x)} \\mathrm{d}x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用Jensen不等式很容易得到 $D(Q\\|P)\\geq 0$，当且仅当$Q=P$时，$D(Q\\|P)= 0$  \n",
    "\n",
    "$$\n",
    "\\begin{aligned} - D ( Q \\| P ) & = \\int Q ( x ) \\log \\frac { P ( x ) } { Q ( x ) } \\mathrm { d } x \\\\ & \\leqslant \\log \\int Q ( x ) \\frac { P ( x ) } { Q ( x ) } \\mathrm { d } x \\\\ & = \\log \\int P ( x ) \\mathrm { d } x = 0 \\end{aligned} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.4.1 变分推理（variational inference）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. MCMC通过<font color=blue>随机抽样</font>的方法近似地计算模型的后验概率；\n",
    "\n",
    "\n",
    "2. VI通过解析的方法计算模型的后验概率的近似值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VI的基本思想：**\n",
    "\n",
    "假设模型是联合概率分布$~p(x,z)$，其中$~x~$是观测变量（数据），$~z~$是隐变量，包括参数。  \n",
    "\n",
    "目标是学习模型的后验分布$~p(z|x)$，用模型进行概率推理。\n",
    "\n",
    "VI考虑用概率分布$~q(z)~$近似条件分布$~p(z|x)$，用KL散度$D\\left(q(z)\\| p(z|x)\\right)$计算两者的相似度，$~q(z)~$称为<font color=blue>变分分布（variational distribution）</font>\n",
    "\n",
    "如果能找到与$~p(z|x)~$在KL散度意义下最近的分布$~q^{*}(z)~$，则可以用这个分布近似$~p(z|x)~$  \n",
    "\n",
    "$$\n",
    "p(z|x) \\approx q^{*}(z)\n",
    "$$\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{eqnarray}\n",
    "D\\left(q(z)\\| p(z|x)\\right) &=& \\int q(z) \\log \\frac{q(z)}{p(z|x)} \\mathrm{d}z \\\\\n",
    "&=& \\int q(z) \\log q(z) \\mathrm{d}z - \\int q(z) \\log p(z|x) \\mathrm{d}z \\\\\n",
    "&=& \\int q(z) \\log q(z) \\mathrm{d}z - \\int q(z) \\log p(x,z) \\mathrm{d}z + \\int q(z) \\log p(x) \\mathrm{d}z \\\\\n",
    "&=& E_{q}\\left[\\log q(z)\\right] - E_{q}\\left[\\log p(x,z)\\right] + \\log p(x) \\\\\n",
    "&=& \\log p(x) - \\left\\{\\underset{ELBO}{\\underbrace{ E_{q}\\left[\\log p(x,z)\\right] - E_{q}\\left[\\log q(z)\\right]} }\\right\\} \\tag{20.35} \\\\\n",
    "&\\geq& 0\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此有  \n",
    "\n",
    "$$\n",
    "\\underset{evidence}{\\underbrace{\\log p(x)}} \\geq  \\underset{\\text{evidence  lower  bound, ELBO}}{\\underbrace{E_{q}\\left[\\log p(x,z)\\right] - E_{q}\\left[\\log q(z)\\right]}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "证据下界记作: \n",
    "\n",
    "$$\n",
    "L(q) = E_{q}\\left[\\log p(x,z)\\right] - E_{q}\\left[\\log q(z)\\right] \\tag{20.37}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
