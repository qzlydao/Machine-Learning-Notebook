{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='23_附2_优化方法汇总.jpg' style='zoom:100%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 方向导数与梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 方向导数定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='23_方向导数.png' style='zoom:50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设$l$是$xOy$平面上以$P_{0}(x_{0},y_{0})$为始点的一条射线，$\\mathbf{e}_{l}=(\\cos \\alpha, \\cos \\beta)$是与$l$同方向的单位向量，射线方程为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x=x_{0} + t\\cos \\alpha, \\\\\n",
    "y=y_{0} + t\\cos \\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设函数$z=f(x,y)$在点$P_{0}(x_{0},y_{0})$的某个领域$U(P_{0})$内有定义，$P(x_{0} + t\\cos \\alpha, y_{0} + t\\cos \\beta)$为$l$上另一点，且$P \\in U(P_{0})$.如果函数增量$f(x_{0} + t\\cos \\alpha, y_{0} + t\\cos \\beta)-f(x_{0},y_{0})$与$P$到$P_{0}$的距离$|PP_{0}|=t$的比值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{f(x_{0} + t\\cos \\alpha, y_{0} + t\\cos \\beta)-f(x_{0},y_{0})}{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当$P$沿着$l$趋近于$P_{0}$时的极限存在，则称此极限为函数$f(x,y)$在点$P_{0}$沿方向$l$的<font color=red>方向导数</font>，记作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left.\\frac{\\partial f}{\\partial l}\\right|_{(x_{0},y_{0})} = \\lim_{t\\to 0^{+}}\\frac{f(x_{0} + t\\cos \\alpha, y_{0} + t\\cos \\beta)-f(x_{0},y_{0})}{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 方向导数计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left.\\frac{\\partial f}{\\partial l}\\right|_{(x_{0},y_{0})} = f_{x}(x_{0},y_{0})\\cos \\alpha + f_{y}(x_{0},y_{0})\\cos \\beta\n",
    "$$\n",
    "  \n",
    "$\\cos \\alpha,\\cos \\beta$是方向$l$的方向余弦。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 梯度（是个向量）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数在点$P_{0}(x_{0},y_{0})$处的梯度:  \n",
    "  \n",
    "  \n",
    "$$\n",
    "\\mathbf{grad} f(x_{0},y_{0}) = \\nabla f(x_{0},y_{0}) = f_{x}(x_{0},y_{0})\\mathbf{i} + f_{y}(x_{0},y_{0})\\mathbf{j} = \\left( f_{x}(x_{0},y_{0}), f_{y}(x_{0},y_{0}) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 <font color=blue>方向导数与梯度的关系</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "记$\\mathbf{e}_{l}=(\\cos \\alpha, \\cos \\beta)$是与方向$l$同向的单位向量，则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\left.\\frac{\\partial f}{\\partial l}\\right|_{(x_{0},y_{0})} &=& f_{x}(x_{0},y_{0})\\cos \\alpha + f_{y}(x_{0},y_{0})\\cos \\beta \\\\\n",
    "&=& \\mathbf{grad} f(x_{0},y_{0}) \\cdot \\mathbf{e}_{l} \\\\\n",
    "&=& \\left| \\mathbf{grad} f(x_{0},y_{0}) \\right|\\cos \\theta\n",
    "\\end{eqnarray}\n",
    "$$  \n",
    "  \n",
    "$\\theta=\\left(\\mathbf{grad} f(x_{0},y_{0}), \\mathbf{e}_{l} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）当$\\theta=0$时，方向$\\mathbf{e}_{l}$与梯度$\\mathbf{grad} f(x_{0},y_{0})$方向相同，函数$f(x,y)$在这个方向导数达到最大值，此时方向导数等于梯度的模:  \n",
    "  \n",
    "  $$\n",
    "  \\left.\\frac{\\partial f}{\\partial l}\\right|_{(x_{0},y_{0})}  = \\left| \\mathbf{grad} f(x_{0},y_{0}) \\right|\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）当$\\theta=\\pi$，函数减小最快，在这个方向导数达到最小值   \n",
    "\n",
    "$$\n",
    "\\left.\\frac{\\partial f}{\\partial l}\\right|_{(x_{0},y_{0})}  = -\\left| \\mathbf{grad} f(x_{0},y_{0}) \\right|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（3）当$\\theta=\\pi/2$，方向$\\mathbf{e}_{l}$与梯度$\\mathbf{grad} f(x_{0},y_{0})$方向正交，函数变化为零，即\n",
    "$$\n",
    "\\left.\\frac{\\partial f}{\\partial l}\\right|_{(x_{0},y_{0})} = \\left| \\mathbf{grad} f(x_{0},y_{0}) \\right|\\cos \\theta = 0\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 泰勒级数与泰勒展开"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 泰勒级数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果$f(x)$在点$x=x_{0}$具有任意阶导数，则幂级数:  \n",
    "\n",
    "$$\n",
    "\\sum_{n=0}^{\\infty} \\frac{f^{(n)}\\left(x_{0}\\right)}{n !}\\left(x-x_{0}\\right)^{n}=f\\left(x_{0}\\right)+f^{\\prime}\\left(x_{0}\\right)\\left(x-x_{0}\\right)+\\frac{f^{\\prime \\prime}\\left(x_{0}\\right)}{2 !}\\left(x-x_{0}\\right)^{2}+\\cdots+\\frac{f^{(n)}\\left(x_{0}\\right)}{n !}\\left(x-x_{0}\\right)^{n}+\\cdots\n",
    "$$  \n",
    "\n",
    "称为$f(x)$在点$x=x_{0}$处的泰勒级数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 泰勒公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若函数$f(x)$在包含$x_{0}$的某个闭区间$[a,b]$上具有$n$阶导数，且在开区间$(a,b)$上具有$(n+1)$阶导数，则对闭区间$[a,b]$上任意一点$x$，下式成立:  \n",
    "\n",
    "$$\n",
    "f(x) = \\frac{f\\left(x_{0}\\right)}{0!}+ \\frac{f^{\\prime}\\left(x_{0}\\right)}{1!}\\left(x-x_{0}\\right)+\\frac{f^{\\prime \\prime}\\left(x_{0}\\right)}{2 !}\\left(x-x_{0}\\right)^{2}+\\cdots+\\frac{f^{(n)}\\left(x_{0}\\right)}{n !}\\left(x-x_{0}\\right)^{n}+ R_{n}(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R_{n}(x)$是泰勒公式的余项。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 泰勒展开式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x) = \\frac{f\\left(x_{0}\\right)}{0!}+ \\frac{f^{\\prime}\\left(x_{0}\\right)}{1!}\\left(x-x_{0}\\right)+\\frac{f^{\\prime \\prime}\\left(x_{0}\\right)}{2 !}\\left(x-x_{0}\\right)^{2}+\\cdots+\\frac{f^{(n)}\\left(x_{0}\\right)}{n !}\\left(x-x_{0}\\right)^{n}+ \\cdots\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 算法思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设$f(x)$是$\\mathbf{R}^{n}$上具有<font color=red>一阶连续偏导数</font>的函数，有求解的无约束最优化问题是\n",
    "$$\n",
    "\\min_{x} \\quad f(x)\n",
    "$$\n",
    "$x^{*}$表示目标函数$f(x)$的极小点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GD的思想\n",
    "GD是一种迭代算法。选取适当的初值$x^{(0)}$，不断迭代，更新$x$值，进行目标的最小化，直到收敛。<font color=blue>由于负梯度是使函数值下降最快的方向</font>，在迭代的每一步，以负梯度方向更新$x$的值，从而达到减小函数值的目的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 算法细节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于$f(x)$具有一阶连续偏导数，若第$k$次迭代值为$x^{(k)}$，则可将$f(x)$在$x^{(k)}$附近进行一阶泰勒展开:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x) = f(x^{(k)}) + g_{k}^{T}(x-x^{(k)})\n",
    "$$\n",
    "\n",
    "这里，$g_{k} = g(x^{(k)}) = \\nabla f(x^{(k)})$为$f(x)$在$x^{(k)}$处的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们求出第$k+1$次的迭代值$x^{(k+1)}$:\n",
    "\n",
    "$$\n",
    "x^{(k+1)} \\leftarrow x^{(k)} + \\lambda_{k}p_{k}\n",
    "$$\n",
    "\n",
    "<font color=red>???</font>其中$p_{k}$是搜索方向，取负梯度方向$p_{k} = - \\nabla f(x^{(k)})$，$\\lambda_{k}$是步长，需满足：\n",
    "\n",
    "$$\n",
    "f\\left(x^{(k)}+\\lambda_{k} p_{k}\\right)=\\min _{\\lambda \\geqslant 0} f\\left(x^{(k)}+\\lambda p_{k}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 GD描述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入: 目标函数$f(x)$，梯度函数$g(x)=\\nabla f(x)$，计算精度$\\epsilon$；  \n",
    "  \n",
    "  \n",
    "输出: $f(x)$的极小点$x^{*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 取初始值$x^{(0)}$，置$k=0$  \n",
    "  \n",
    "  \n",
    "(2) 计算$f(x^{(k)})$  \n",
    "  \n",
    "  \n",
    "(3) 计算梯度$g_{k} = g(x^{(k)})$，当$\\| g_{k} \\| < \\epsilon$时，停止迭代，令$x^{*} = x^{(k)}$；否则，令$p_{k} = -g(x^{(k)})$，求$\\lambda_{k}$，使\n",
    "\n",
    "$$\n",
    "f\\left(x^{(k)}+\\lambda_{k} p_{k}\\right)=\\min _{\\lambda \\geqslant 0} f\\left(x^{(k)}+\\lambda p_{k}\\right)\n",
    "$$\n",
    "\n",
    "(4) 置$x^{(k+1)} = x^{(k)} + \\lambda_{k}p_{k}$，计算$f(x^{(k+1)})$  \n",
    "      当$\\| f(x^{(k+1)}) - f(x^{(k)}) \\| < \\epsilon$或$\\| x^{(k+1)} - x^{(k)} \\| < \\epsilon $时，停止迭代，令$x^{*} = x^{(k)}$；  \n",
    "  \n",
    "  \n",
    "(5) 否则，置$k+=1$，转(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def fun(x):\n",
    "    return 0.5 * (x-0.25)**2\n",
    "\n",
    "def fg(x):\n",
    "    return 0.5 * 2 * (x-0.25)\n",
    "\n",
    "\n",
    "class GrandDescent():\n",
    "    \n",
    "    def __init__(self, x, lr, epsilon, max_iter):\n",
    "        self.x = x\n",
    "        self.f_current = fun(x)\n",
    "        self.f_gd = fg(x)\n",
    "        self.f_change = sys.maxsize\n",
    "        self.lr = lr \n",
    "        self.epsilon = epsilon\n",
    "        self.max_iter = max_iter\n",
    "        self.iter_num = 0\n",
    "        \n",
    "    def optim(self):\n",
    "        while self.f_change > self.epsilon and self.iter_num < self.max_iter and self.f_gd != 0:\n",
    "            self.iter_num += 1\n",
    "            \n",
    "            # 计算梯度\n",
    "            gd = fg(self.x)\n",
    "            # 更新x\n",
    "            self.x -= self.lr * gd\n",
    "            # 计算更新x后的值\n",
    "            f_tmp = fun(self.x)\n",
    "            # 计算变化量\n",
    "            self.f_change = self.f_current - f_tmp\n",
    "            self.f_current = f_tmp\n",
    "            \n",
    "        \n",
    "    def output(self):\n",
    "        print('optimal x:',self.x, ' optimal y:', self.f_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal x: 0.2500984594516654  optimal y: 4.847131811127832e-09\n"
     ]
    }
   ],
   "source": [
    "gd = GrandDescent(5,0.01,1e-10,10000)\n",
    "gd.optim()\n",
    "gd.output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 批量梯度下降法（Batch Gradient Descent，BGD）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 随机梯度下降（Stochastic Gradient Descent，SGD）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
