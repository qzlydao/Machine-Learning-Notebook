{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二章 感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 感知机（perceptron）是<font color=blue>二分类的线性分类模型；</font>\n",
    "\n",
    "\n",
    "2. 感知机将输入空间（特征空间）中的实例划分为正负两类的分离超平面，属于<font color=blue>判别模型</font>；\n",
    "\n",
    "\n",
    "3. 感知机旨在<font color=blue>求出将训练数据进行线性划分的分离超平面，</font>为此，导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，从而求得感知机模型；\n",
    "\n",
    "\n",
    "4. 感知机模型是神经网络和SVM的基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 感知机模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 符号函数sign\n",
    "\n",
    "$$\n",
    "\\text{sign} = \\begin{cases}\n",
    "+1  & \\text{ if } x \\geq 0 \\\\\n",
    "-1  & \\text{ if } x < 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义\n",
    "输入空间（特征空间）是$\\mathcal{X} \\subseteq  \\mathbf{R}^{n}$，$x \\in \\mathcal{X}$表示输入空间实例的特征向量；\n",
    "\n",
    "\n",
    "输出空间是$\\mathcal{Y} = \\{+1, -1\\}$，输出$y \\in \\mathcal{Y}$表示实例的类别。\n",
    "\n",
    "由输入空间到输出空间的映射函数如下：\n",
    "\n",
    "$$\n",
    "f(x) = \\text{sign}(w \\cdot x +b)\n",
    "$$\n",
    "\n",
    "称之为感知机。\n",
    "\n",
    "感知机模型的<font color=red>假设空间</font>是定义在特征空间中的所有线性分类器模型，即$\\{ f| f(x) = w \\cdot x +b\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 感知机的几何解释\n",
    "\n",
    "<img src='2.1_感知机模型.png' style='zoom:50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 感知机学习策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 数据的线性可分性\n",
    "\n",
    "给定一个数据集\n",
    "$$\n",
    "T = \\{ (x_{1}, y_{1}), (x_{2}, y_{2}), \\cdots, (x_{N}, y_{N}) \\}\n",
    "$$\n",
    "如果存在某个超平面$S$能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，则称数据集$T$是<font color=blue>线性可分数据集（linearly separable data set）</font>；否则，称数据集$T$线性不可分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 感知机学习策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机采用的（经验）损失函数是：<font color=blue>误分类点在分离超平面$S$的总距离最小。</font>\n",
    "\n",
    "首先，输入空间中任意一点$x_{0}$到超平面$S$的距离为：\n",
    "$$\n",
    "\\frac{1}{\\left\\| w \\right\\|} |w \\cdot x_{0} + b|\n",
    "$$\n",
    "\n",
    "这里，$\\left\\| w \\right\\|$是$w$的$L_{2}$范数。\n",
    "\n",
    "其次，对于误分类的点$(x_{i},y_{i})$，下式始终成立：\n",
    "$$\n",
    "-y_{i}(w\\cdot x_{i} + b) > 0 , \\quad \\quad y_{i} = \\pm 1\n",
    "$$\n",
    "\n",
    "带入上式，得到误分类点$x_{i}$到超平面$S$的距离是：\n",
    "$$\n",
    "- \\frac{1}{\\left\\| w \\right\\|}y_{i}(w\\cdot x_{i} + b)\n",
    "$$\n",
    "\n",
    "假设误分类点集合为$M$，那么所有误分类点到超平面的距离为\n",
    "$$\n",
    "- \\frac{1}{\\left\\| w \\right\\|} \\sum_{x_{i} \\in M} y_{i}(w\\cdot x_{i} + b)\n",
    "$$\n",
    "\n",
    "$w$只决定超平面的方向，这里不考虑$\\frac{1}{\\left\\| w \\right\\|} $，于是得到感知机学习的损失函数：\n",
    "$$\n",
    "\\color{red}{L(w,b) = - \\sum_{x_{i} \\in M} y_{i}(w\\cdot x_{i} + b)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 补充：点到超平面$S$的距离\n",
    "\n",
    "<img src='2.2_点到超平面的距离.png' style='zoom:50%' />\n",
    "\n",
    "$x_{1}$是$x_{0}$在超平面的投影点，所以\n",
    "$$\n",
    "\\overrightarrow{x_{1}x_{0}} = \\overrightarrow{x_{0}} - \\overrightarrow{x_{1}} \\quad  // \\quad \\overrightarrow{w} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "|w \\cdot \\overrightarrow{x_{1}x_{0}} | &=& |w \\cdot (x_{0} - x_{1})| \\\\\n",
    "&=& |w \\cdot x_{0} + b - w \\cdot x_{1} -b| \\\\\n",
    "&=& |w \\cdot x_{0} + b|\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "又\n",
    "$$\n",
    "|w \\cdot \\overrightarrow{x_{1}x_{0}} | = |w||\\overrightarrow{x_{1}x_{0}}| = \\left\\| w \\right \\| d\n",
    "$$\n",
    "所以：\n",
    "$$\n",
    "d = \\frac{1}{\\left\\| w \\right \\| }|w \\cdot x_{0} + b|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 感知机学习算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 感知机学习算法的原始形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机学习算法其实就是对损失函数极小化的最优化问题：\n",
    "$$\n",
    "\\min_{w,b} = - \\sum_{x_{i} \\in M} y_{i}(w \\cdot x_{i} +b) \\tag{2.5}\n",
    "$$\n",
    "$M$为误分类点的集合。\n",
    "\n",
    "具体采用<font color=blue>随机梯度下降法（stochastic gradient descent，SGD）：</font>\n",
    "\n",
    "<font color=BlueViolet>首先，任意选取一个超平面$w_{0},b_{0}$；</font>\n",
    "\n",
    "<font color=BlueViolet>然后，利用SGD不断地极小化目标函数（2.5）</font>\n",
    "\n",
    "<font color=BlueViolet>极小化过程中不是一次使用$M$中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设误分类点集合$M$是固定的，那么损失函数的梯度为：\n",
    "$$\n",
    "\\nabla_{w}L(w,b) = - \\sum_{x_{i} \\in M}y_{i}x_{i} \\\\\n",
    "\\nabla_{b}L(w,b) = - \\sum_{x_{i} \\in M}y_{i}\n",
    "$$\n",
    "\n",
    "随机选取一个误分类点$(x_{i}, y_{i})$，对$w,b$进行更新：<font color=red>因为是根据一个误分类样本进行更新参数，所以没了$\\sum$</font>\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "w &\\gets& w+\\eta y_{i}x_{i} \\tag{2.6}\\\\\n",
    "b &\\gets& \\eta y_{i} \\tag{2.7}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "其中$\\eta(0<\\eta \\leq 1)$ 是学习率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 算法描述\n",
    "\n",
    "输入：训练数据$T$，学习率$\\eta$；\n",
    "\n",
    "输出：$w, b$；感知机模型$f(x) = \\text{sign}(w \\cdot x+b)$\n",
    "\n",
    "(1)选取初值$w_{0}, b_{0}$；\n",
    "\n",
    "(2)在训练集中选取数据$(x_{i}, y_{i})$；\n",
    "\n",
    "(3)如果$y_{i}(w \\cdot x_{i} +b) \\leq 0$:\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "w &\\gets& w+\\eta y_{i}x_{i} \\\\\n",
    "b &\\gets& \\eta y_{i}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "(4)转至(2)，直至训练集中没有误分类点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>感知机学习算法由于采用不同的初值或选取不同的误分类点，解可以不同。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 算法的收敛性\n",
    "\n",
    "记符号$\\hat{w} = (w^{T},b)^{T}$，$\\hat{x} = (x^{T}, 1)^{T}$\n",
    "\n",
    "**定理：**  \n",
    "设训练集$T=\\{(x_{1}, y_{1}), (x_{2}, y_{2}), \\cdots, (x_{N}, y_{N}), \\}$ 是线性可分的，则  \n",
    "\n",
    "（1）存在满足条件$\\left \\| \\hat{w}_{opt} \\right\\| = 1$的超平面$\\hat{w}_{opt} \\cdot \\hat{x} = w_{opt}\\cdot x + b_{opt}=0$将训练数据完全正确分开；且存在$\\gamma > 0$，对所有$i=1,2,\\cdots, N$，满足\n",
    "$$\n",
    "y_{i}(\\hat{w}_{opt} \\cdot \\hat{x}) = y_{i}(w_{opt}\\cdot x + b_{opt}) \\geq \\gamma\n",
    "$$\n",
    "\n",
    "（2）令$R = \\max_{1 \\leq i \\leq N} \\left \\| \\hat{x}_{i} \\right\\|$ ，则感知机算法在训练数据集上的误分类次数$k$满足不等式:\n",
    "$$\n",
    "k \\leq \\left( \\frac{R}{\\gamma} \\right)^{2}\n",
    "$$\n",
    "\n",
    "\n",
    "**证明：**(略，见书)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 感知机学习算法的对偶形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对偶形式的基本思想是：<font color=blue>将$w, b$表示为实例$x_{i}$和标记$y_{i}$的线性组合，通过求解其系数而求得$w,b$。</font>\n",
    "\n",
    "对于原始形式的学习算法，假设初始值$w_{0}=0，b_{0}=0$.对误分类点$(x_{i}, y_{i})$，通过$n_{i}$次如下更新，从而逐步修改$w，b$:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "w &\\gets& w+\\eta y_{i}x_{i} \\\\\n",
    "b &\\gets& \\eta y_{i}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "设$\\alpha_{i} = n_{i}\\eta$，则$w,b$关于$(x_{i}, y_{i})$的增量分别是$\\alpha_{i}y_{i}x_{i}$和$\\alpha_{i}y_{i}$.对于训练集$T$中的$N$个样本，最后学习到的$w,b$可表示为：\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "w &=& \\sum_{i=1}^{N} n_{i}\\eta y_{i}x_{i} = \\sum_{i=1}^{N} \\alpha_{i} y_{i}x_{i} \\tag{2.14} \\\\\n",
    "b &=& \\sum_{i=1}^{N} n_{i}\\eta y_{i} = \\sum_{i=1}^{N} \\alpha_{i} y_{i} \\tag{2.15}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "$n_{i}$越大，表示样本$(x_{i}, y_{i})$更新次数也越多，意味着它距离超平面$S$越近，也就越难正确分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 算法描述\n",
    "\n",
    "输入：$T = \\{ (x_{1}, y_{1}), (x_{2}, y_{2}), \\cdots, (x_{N}, y_{N}) \\}$和学习率$\\eta$；\n",
    "\n",
    "输出：$\\alpha=(\\alpha_{1}, \\alpha_{2}, \\cdots, \\alpha_{N})^{T}, b$；感知机模型$f(x) = \\text{sign}\\left(\\sum_{j=1}^{N} \\alpha_{j}y_{j}x_{j} \\cdot x+b \\right)$\n",
    "\n",
    "(1) $\\alpha \\gets \\mathbf{0}, b \\gets 0$\n",
    "\n",
    "(2) 在训练集中选取数据$(x_{i}, y_{i})$\n",
    "\n",
    "(3) 如果$y_{i}\\left(\\sum_{j=1}^{N} \\alpha_{j}y_{j}x_{j} \\cdot x_{i} + b \\right) \\leq 0$，则\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "n_{i} &\\gets& n_{i}+1 \\\\\n",
    "\\alpha_{i} &\\gets& (n_{i}+1)\\eta = \\alpha_{i} + \\eta \\\\\n",
    "b_{i} = \\alpha_{i}y_{i} &\\gets& (\\alpha_{i} + \\eta)y_{i} = b_{i} + \\eta y_{i}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "    <font color=red>本质上更新的是$n_{i}$，也就是次数。</font>\n",
    "\n",
    "(4)转到(2)直到没有误分类数据。\n",
    "\n",
    "<font color='BlueViolet'>对偶形式中训练实例仅以内积形式出现，为了方便，可以预先将训练集中实例间的内积计算出来并以矩阵形式存储，这个矩阵就是**Gram矩阵** </font>\n",
    "$$\n",
    "G = [x_{i} \\cdot x_{j}]_{N \\times N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 <font color=red>原始形式 VS. 对偶形式</font>\n",
    "\n",
    "1. 首先，原始形式和对偶形式是等价的；\n",
    "\n",
    "\n",
    "2. 对偶形式的目的是降低每次迭代的运算量，但不是在任何情况下都能降低运算量，<font color=red>而是在特征空间的维度远大于数据集大小时才起作用。</font>\n",
    "\n",
    "\n",
    "不妨设特征空间是$\\mathcal{R}^{n}$，训练集大小为$N$，$N \\ll n$。在原始形式的感知机学习算法中，每一轮迭代中我们至少都要判断某个输入实例是不是误判点，即对于$(x_{i}, y_{i})$，看是否有$y_{i}(w \\cdot x_{i} +b) \\leq 0$。这里的运算量主要集中在求内积$w \\cdot x_{i}$，由于特征空间维度很高，所以很慢。\n",
    "\n",
    "\n",
    "而在对偶形式的学习算法中，对于$(x_{i}, y_{i})$，误判条件变为：$y_{i}\\left(\\sum_{j=1}^{N} \\alpha_{j}y_{j}x_{j} \\cdot x_{i} + b \\right) \\leq 0$。这里所有输入实例都仅以内积形式出现，所以我们可以预先计算输入实例的Gram矩阵。这样每次做误判检测的时候我们直接在Gram矩阵里查表就能拿到内积$x_{j} \\cdot x_{i}$，所以误判检测的时间复杂度是$O(N)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, w, b, rate=1.0, max_iter=10000):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.rate = rate\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "    # 原始形式学习算法\n",
    "    def fit(self, X, Y):\n",
    "        iter_count = 0\n",
    "        while iter_count <= self.max_iter:\n",
    "            for idx, x in enumerate(X):\n",
    "                tmp = Y[idx] * (self.w.dot(x) + self.b)\n",
    "                if tmp <= 0:\n",
    "                    self.w = np.add(self.w, self.rate*Y[idx]*x)\n",
    "                    self.b = self.b + self.rate * Y[idx]\n",
    "                    break\n",
    "                if idx == len(X)-1:\n",
    "                    return self.w, self.b\n",
    "            iter_count += 1\n",
    "        else:\n",
    "            print('迭代次数达到上限%d'%self.max_iter)\n",
    "    \n",
    "    # 对偶学习算法\n",
    "    def fit_dual(self, X, Y):\n",
    "        N = len(X) # 数据集大小\n",
    "        \n",
    "        # 定义alpha向量\n",
    "        alpha = np.zeros(N)\n",
    "        \n",
    "        # 计算Gram矩阵\n",
    "        gram = Perceptron.get_gram(X)\n",
    "        \n",
    "        # 迭代更新alpha_i 和 b\n",
    "        iter_count = 0\n",
    "        while iter_count <= self.max_iter:\n",
    "            for i in range(N):\n",
    "                tmp = 0.0\n",
    "                for j in range(N):\n",
    "                    tmp += alpha[j]*Y[j]*gram[j][i]\n",
    "                if Y[i]*(tmp+self.b) <= 0:\n",
    "                    alpha[i] += self.rate\n",
    "                    self.b +=  self.rate*Y[i]\n",
    "                    break\n",
    "            else:\n",
    "                return self.w, self.b\n",
    "        else:\n",
    "            print('迭代次数达到上限%d'%self.max_iter)\n",
    "       \n",
    "    @staticmethod\n",
    "    def get_gram(X):                                                                                                                                                            \n",
    "        N = len(X)\n",
    "        gram = np.zeros((N, N))\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                gram[i][j] = np.dot(X[i], X[j])\n",
    "        return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n",
      "-3\n",
      "\n",
      "[1 1]\n",
      "-3\n"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [3, 3],\n",
    "    [4, 3],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "Y = np.array([1, 1, -1])\n",
    "\n",
    "w = np.array([0, 0])\n",
    "\n",
    "perceptron = Perceptron(w, 0, 1)\n",
    "w, b = perceptron.fit(X, Y)\n",
    "print(w)\n",
    "print(b)\n",
    "print()\n",
    "\n",
    "p2 = Perceptron(w, 0, 1)\n",
    "w, b = p2.fit_dual(X, Y)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
