{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 有向图与无向图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[概率图模型](https://zhuanlan.zhihu.com/p/54101808)\n",
    "\n",
    "概率图模型三大理论部分：\n",
    "\n",
    "1. 表示\n",
    "    1. 有向图（离散）：贝叶斯网络\n",
    "    \n",
    "    2. 高斯图（连续）：高斯贝叶斯和高斯Markov网络\n",
    "    \n",
    "    3. 无向图（离散）：Markov网络\n",
    "    \n",
    "    \n",
    "2. 推断\n",
    "\n",
    "    1. 精确推断\n",
    "        - 变量消去法\n",
    "        \n",
    "        - 信念传播法\n",
    "        \n",
    "    2. 近似推断\n",
    "        - 确定性近似（VI）\n",
    "        \n",
    "        - 随机近似（MCMC）\n",
    "        \n",
    "        \n",
    "3. 学习\n",
    "    1. 参数学习\n",
    "        - 完备数据\n",
    "        \n",
    "        - 隐变量：EM法\n",
    "        \n",
    "    2. 结构学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 有向图——贝叶斯网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已知联合分布中，各个随机变量之间的依赖关系，那么可以通过拓扑排序（根据依赖关系）可以获得一个有向图。而如果已知一个图，也可以直接得到联合概率分布的因子分解：\n",
    "\n",
    "$$p\\left(x_{1}, x_{2}, \\cdots, x_{p}\\right)=\\prod_{i=1}^{p} p\\left(x_{i} \\mid x_{\\text {parent }(i)}\\right)$$\n",
    "\n",
    "图中条件独立性是如何体现的呢？在局部任何三个结点，可以构成三种结构：\n",
    "\n",
    "<table>\n",
    "    <td><img src='attachment/27. 有向图_局部结构1.png'/></td>\n",
    "    <td><img src='attachment/27. 有向图_局部结构2.png'/></td>\n",
    "    <td><img src='attachment/27. 有向图_局部结构3.png'/></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 无向图——Markov网络（Markov Random Field）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Markov网络中，也存在D划分的概念：\n",
    "- 全局马尔可夫性\n",
    "- 局部马尔可夫性\n",
    "- 成对马尔科夫性\n",
    "\n",
    "联合概率可以分解为最大团上势函数的乘积形式：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x) &= \\frac{1}{Z}\\prod_{i=1}^{K}\\phi(x_{ci}) \\\\\n",
    "Z &= \\sum_{x\\in \\mathcal{X}} \\prod_{i=1}^{K}\\phi(x_{ci}) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "其中$\\phi(x_{ci})$叫做势函数，它必须是一个正值，可以记为：\n",
    "\n",
    "$$\n",
    "\\phi(x_{ci}) = \\exp(-E(x_{ci}))\n",
    "$$\n",
    "\n",
    "这个分布叫作Gibbs分布（玻尔兹曼分布），这个分解和条件独立性等价（Hammesley-Clifford定理），这个分布的形式也和指数族分布形式上相同，于是满足最大熵原理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 推断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过概率图模型，我们得到了唯一的联合概率分布，通过联合概率分布我们一般可以计算变量的边缘分布和条件分布，其中计算条件分布的过程即对应推断任务。\n",
    "\n",
    "通常推断可以分为：\n",
    "\n",
    "1. 精确推断\n",
    "    - Variable Elimination（VE）\n",
    "    - Belief Propagation（BP，Sum-Product Algorithm）\n",
    "    - Junction Tree，上面两种在树结构上应用，Junction Tree在图结构上应用\n",
    "    \n",
    "2. 近似推断\n",
    "    - Mente Carlo Interference，如 Importance Sampling，MCMC\n",
    "    - Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 推断——变量消除（VE）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "变量消除的方法是在求解概率分布的时候，<font color=blue>将相关的条件概率先行求和或积分，从而一步步地消除变量</font>，例如在马尔科夫链中：\n",
    "\n",
    "\n",
    "<img src='27. 马尔科夫链.png' style='zoom:50%'/>\n",
    "\n",
    "变量消除的缺点很明显：\n",
    "\n",
    "1. 计算步骤无法存储\n",
    "\n",
    "2. 消除的最优次序是一个NP-hard问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 推断——信念传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 推断——Max-Product算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 推断——MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在现实的许多情况下我们需要计算某些变量的和或积分时，由于其数目较多，直接计算行不通。这是可以引入蒙特卡洛采用的方法，<font color=blue>将求和或积分运算转化为求某一特定分布下的期望，进而通过平均值近似期望得到所求结果。</font>\n",
    "\n",
    "此外，如果样本服从i.i.d分布，则根据大数定律，其估值均值必将收敛至期望值。这也保证了蒙特卡洛采样的合理性和估计的无偏性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 推断——变分推断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='attachment/27. HMM、MEMM到CRF关系.png' style='zoom:50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 生成 VS. 判别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 判别模型\n",
    "\n",
    "    判别方法由数据直接学习决策函数$f(x)$ 或者条件概率分布 $P(y \\mid x)$ 作为预测的模型，即判别模型。\n",
    "\n",
    "    判别方法关心的是对给定输入 $x$ ，应该预测什么样的输出 $y$ 。\n",
    "\n",
    "\n",
    "2. 生成模型\n",
    "\n",
    "    生成方法由数据学习输入和输出的联合概率分布 $P(x,y)$，然后求出后验概率分布 $P(y\\mid x)$ 作为预测的模型，即生成模型。以Naive Bayes为例，我们要求的目标可以通过：\n",
    "    \n",
    "    $$\n",
    "    P(x, y) = P(x \\mid y) P(y)\n",
    "    $$\n",
    "    \n",
    "    求出输入输出的联合概率分布，然后通过贝叶斯公式：\n",
    "    \n",
    "    $$\n",
    "    P(y \\mid x) = \\frac{P(x \\mid y)P(y)}{P(x)}\n",
    "    $$\n",
    "    \n",
    "    求出后验概率分布。\n",
    "    \n",
    "    \n",
    "3. 判别模型优缺点\n",
    "    \n",
    "    - 优点：\n",
    "    \n",
    "        - 仅需要有限的样本。节省计算资源，需要的样本数量也少于生成模型。\n",
    "        \n",
    "        - 由于直接学习$P(y\\mid x)$，而不需要求解类别条件概率，所以允许我们对输入进行抽象（比如降维、构造），从而能够简化学习问题。\n",
    "        \n",
    "    - 缺点：\n",
    "    \n",
    "        - 不能反映训练数据本身的特性。能力有限，可以告诉你的是1还是2，但没有办法把整个场景描述出来。\n",
    "        \n",
    "        - 黑盒操作：变量间的关系不清楚，不可见。\n",
    "\n",
    "\n",
    "4. 生成模型优缺点\n",
    "    \n",
    "    - 优点：\n",
    "        \n",
    "        - 生成模型给出的是联合分布 $P(x,y)$，不仅能够由联合分布计算后验分布 $P(y\\mid x)$，还可以给出其它信息，比如可以使用 $P(x) = \\sum_{i=1}^{k}P(x\\mid y_{i})P(y_{i})$ 来计算边缘分布 $P(x)$。如果一个输入样本的边缘分布 $P(x)$ 很小的话，那么可以认为学习出的这个模型可能不太适合对这个样本进行分类，分类效果可能不好，这也是所谓的异常值检测（outlier detection）。\n",
    "        \n",
    "        - 生成模型收敛速度比较快，即当样本数量较多时，生成模型能更快地收敛于真实模型；\n",
    "        \n",
    "        - 生成模型能够应付存在隐变量的情况，比如GMM就是含有隐变量的生成方法；\n",
    "        \n",
    "     - 缺点\n",
    "     \n",
    "         - 联合分布能够提供更多的信息，但也需要更多的样本和更多计算，尤其是为了更准确估计类别条件分布，需要增加样本的数目，而且类别条件概率的许多信息是我们做分类用不到，因而如果我们只需要做分类任务，就浪费了计算资源；\n",
    "         \n",
    "         - 另外，实践中多数情况下判别模型效果更好。\n",
    "\n",
    "5. 典型的生成模型：\n",
    "\n",
    "    - Naive Bayes\n",
    "    - HMM\n",
    "\n",
    "\n",
    "6. 典型的判别模型\n",
    "\n",
    "    - k-nn\n",
    "    - perceptron\n",
    "    - decision tree\n",
    "    - Logistic Regression\n",
    "    - Maximum Entropy Model\n",
    "    - SVM\n",
    "    - CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. HMM—>MEMM—>CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 HMM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='attachment/27. HMM.png' style='zoom:50%'/>\n",
    "\n",
    "对联合概率进行建模\n",
    "\n",
    "$$\n",
    "P(x,z) = \\prod_{i=1}^{T}P(z_{i}\\mid z_{i-1})P(x_{i}\\mid z_{i})\n",
    "$$\n",
    "\n",
    "可以看到，在第 $t$ 时刻，观测状态仅依赖当前时刻的隐状态，因此在做序列标注任务时，无法考虑上下文信息。\n",
    "\n",
    "**对HMM进行改进**\n",
    "\n",
    "<img src='attachment/27. HMM改进.png' style='zoom:50%'/>\n",
    "\n",
    "$$\n",
    "P(x,z) = \\prod_{i=1}^{T}P(z_{i} \\mid z_{i-1})\\cdot \\underset{计算困难}{ P(x\\mid z_{1},z_{2},z_{3},z_{4})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 MEMM（最大熵马尔科夫模型）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='attachment/27. MEMM.png' style='zoom:50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "根据有向图的D-划分，HMM中 $x_{t-1}$与 $x_{t}$ 是没有关系的（观测独立假设），而在MEMM中，$x_{t-1}$与 $x_{t}$是有关系的。\n",
    "\n",
    "模型直接对条件概率进行建模：\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "P(z\\mid x) &=& \\prod_{i=1}^{T}P(z_{i}\\mid z_{i-1},x) \\tag{1} \\\\\n",
    "&=& P(z_{1}|x)P(z_{2}|z_{1},x)P(z_{3}|z_{1},z_{2},x)\\cdots P(z_{T}|z_{1},\\cdots,z_{T-1},x) \\tag{2}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "假设标签的依赖只发生在相对位置，所以\n",
    "\n",
    "$$\n",
    "P(z\\mid x) = P(z_{1}|x)P(z_{2}|z_{1},x)P(z_{3}|z_{2},x)\\cdots P(z_{T}|z_{T-1},x) \\tag{3}\n",
    "$$\n",
    "\n",
    "仿照线性链CRF的设计，我们可以设计\n",
    "\n",
    "$$\n",
    "P(z_{1}|x) = \\frac{e^{f(z_{1};x)}}{\\sum_{z_{1}}e^{f(z_{1};x)}}, \\quad P(z_{t}|z_{t-1},x) = \\frac{e^{g(z_{t-1},z_{t}) + f(z_{t};x)}}{\\sum_{z_{t}}e^{g(z_{t-1},z_{t}) + f(z_{t};x)}} \\tag{4}\n",
    "$$\n",
    "\n",
    "将式（4）带入（3）中，得\n",
    "\n",
    "$$\n",
    "P(z_{1}|x) = \\frac{e^{f(z_{1};x) + g(z_{1},z_{2}) + \\cdots + g(z_{T-1},z_{T}) + f(z_{T};x) }}{\\color{red}{\\left(\\sum_{z_{1}}e^{f(z_{1};x)}\\right)\\left( \\sum_{z_{2}}e^{g(z_{1},z_{2}) +f(z_{2};x) } \\right)\\cdots \\left( \\sum_{z_{2}}e^{g(z_{T-1},z_{T}) +f(z_{T};x) } \\right)}}  \\tag{5}\n",
    "$$ \n",
    "\n",
    "有（5）式可以看出，<font color=blue>MEMM与CRF的主要区别就在分母的归一化上，</font>CRF的归一化是<font color=red>全局归一化</font>，而MEMM是<font color=red>局部归一化</font>。\n",
    "\n",
    "#### Label Bias Problem\n",
    "[Label Bias Problem](https://awni.github.io/label-bias/)\n",
    "\n",
    "<img src='attachment/27. Label Bias Problem.png' style='zoom:50%'/>\n",
    "\n",
    "根本原因在于<font color=red>局部归一化</font>\n",
    "\n",
    "<img src='attachment/27. Label Bias Problem2.png' style='zoom:50%'/>\n",
    "\n",
    "可以看到\n",
    "\n",
    "$$\n",
    "P(2|1,i) = 1 = P(2|1)\\\\\n",
    "P(5|4,o) = 1 = P(5|4)\n",
    "$$\n",
    "\n",
    "可以看到，从1->2，以及从4->5，等根本没有考虑观测变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 模型表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogLinear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "P(y|x;w) &= \\frac{\\exp \\left[\\sum_{j=1}^{J}w_{j}f_{j}(x,y) \\right]}{Z(x,w)} \\\\\n",
    "Z(x,w) &= \\sum_{y} \\sum_{j=1}^{J}w_{j}f_{j}(x,y)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear-CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear-CRF也属于log-linear model. 用 $\\bar{x}, \\bar{y}$分别表示输入输出序列\n",
    "\n",
    "$$\n",
    "\\bar{x} = (x_{1}, x_{2}, \\cdots, x_{\\mathrm{T}}) \\\\\n",
    "\\bar{y} = (y_{1}, y_{2}, \\cdots, y_{\\mathrm{T}})\n",
    "$$\n",
    "\n",
    "则，CRF模型可表示为\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(\\bar{y} | \\bar{x};w) &= \\frac{\\exp \\left[\\sum_{j=1}^{J}w_{j}F_{j}(\\bar{x}, \\bar{y}) \\right] }{Z(\\bar{x}, w)} \\\\\n",
    "&= \\frac{\\exp \\left[\\sum_{j=1}^{J}w_{j} \\sum_{i=2}^{\\mathrm{T}} f_{j}(y_{i-1}, y_{i}, \\bar{x})  \\right]} {Z(\\bar{x}, w)}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $w, \\bar{x}$，求出 $\\bar{y}$ \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y} &= \\arg \\max_{\\bar{y}} P(\\bar{y} \\mid \\bar{x};w) \\\\\n",
    "&= \\arg \\max_{\\bar{y}} \\left( \\sum_{j=1}^{J}w_{j} \\sum_{i=2}^{\\mathrm{T}} f_{j}(y_{i-1}, y_{i}, \\bar{x})  \\right) \\\\\n",
    "&= \\arg \\max_{\\bar{y}} \\left( \\sum_{i=2}^{\\mathrm{T}} \\color{MediumOrchid}{ \\sum_{j=1}^{J}w_{j} f_{j}(y_{i-1}, y_{i}, \\bar{x}) }  \\right) \\\\\n",
    "&= \\arg \\max_{\\bar{y}} \\sum_{i=2}^{\\mathrm{T}} \\color{MediumOrchid}{ g_{i}(y_{i-1}, y_{i}, \\bar{x})  }\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "使用Viterbi算法求解\n",
    "\n",
    "假设 $y_{i}$ 的状态有 $m$ 个，则求解的时间复杂度为 $O(m^{2}T)$，空间复杂度为 $O(mT)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般形式的log-linear model参数估计\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\hat{w} &=& \\arg \\max_{w} P(y |x ;w)  \\\\\n",
    "&=& \\arg \\max_{w} \\log P(y |x ;w) \\\\\n",
    "&=& \\arg \\max_{w} \\sum_{j=1}^{J} w_{j}F_{j}(x, y) - \\log Z(x,w) \\tag{1}\\\\ \n",
    "Z(x,w) &=& \\sum_{y^{\\prime}} \\exp \\left[ \\sum_{j=1}^{J}w_{j}F_{j}(x,y^{\\prime}) \\right] \\tag{2}\n",
    "\\end{eqnarray} \n",
    "$$\n",
    "\n",
    "依次对 $w$ 的各分量求偏导，令导数为0\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\log P(y |x ;w)}{\\partial w_{j}} = F_{j}(x, y) - \\frac{1}{Z(x,w)}\\cdot \\boxed{ \\frac{\\partial}{\\partial w_{j}}Z(x,w)} \\tag{3}\n",
    "$$\n",
    "\n",
    "其中\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial w_{j}}Z(x,w) &= \\frac{\\partial}{\\partial w_{j}} \\sum_{y^{\\prime}} \\left[ \\exp \\sum_{j'}^{J} w_{j'}F_{j'}(x, y') \\right] \\\\\n",
    "&= \\sum_{y^{\\prime}} \\frac{\\partial}{\\partial w_{j}} \\left[ \\exp \\sum_{j'}^{J} w_{j'}F_{j'}(x, y') \\right] \\\\\n",
    "&= \\sum_{y^{\\prime}} \\left(\\exp \\sum_{j'}^{J} w_{j'}F_{j'}(x, y')\\right)\\cdot F_{j}(x, y') \\tag{4}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "将（4）式带入（3）式，可得\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\log P(y |x ;w)}{\\partial w_{j}} &=& F_{j}(x, y) - \\frac{1}{Z(x,w)} \\sum_{y^{\\prime}} F_{j}(x, y') \\left(\\exp \\sum_{j'}^{J} w_{j'}F_{j'}(x, y')\\right) \\\\\n",
    "&=& F_{j}(x, y) - \\sum_{y^{\\prime}} F_{j}(x, y') \\frac{\\exp \\sum_{j'}^{J} w_{j'}F_{j'}(x, y')}{Z(x,w)} \\\\\n",
    "&=& F_{j}(x, y) - \\sum_{y^{\\prime}} F_{j}(x, y') \\cdot P(y' | x;w) \\\\\n",
    "&=& F_{j}(x, y) - \\mathbb{E}_{y' \\sim P(y' | x;w)} \\left[ F_{j}(x, y') \\right] \\tag{5}\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Compute Partition Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "归一化因子 $Z(x,w)$ 又称为配分函数。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z(\\bar{x}, w) &= \\sum_{\\bar{y}} \\exp w_{j}F_{j}(\\bar{x}, \\bar{y}) \\\\\n",
    "&= \\sum_{\\bar{y}} \\exp w_{j} \\sum_{i=2}^{\\mathrm{T}} f_{j}(y_{i-1},y_{i},\\bar{x}) \\\\\n",
    "&= \\sum_{\\bar{y}} \\left[ \\exp \\sum_{i=2}^{\\mathrm{T}} g_{i}(y_{i-1},y_{i}) \\right ] \\tag{6}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果直接遍历 $\\bar{y}$ 所有可能的取值，然后求和，则时间复杂度为$O(m^{\\mathrm{T}})$.\n",
    "\n",
    "为了降低计算时间复杂度，利用动态规划（DP）的思想，有前向算法和后向算法。\n",
    "\n",
    "#### Forward Computation（前向算法）\n",
    "\n",
    "定义：\n",
    "\n",
    "$\\alpha(k+1, v)$: `the sum of all possible paths where the tag of time k+1 is v.` 则有递推公式：\n",
    "\n",
    "$$\n",
    "\\alpha(k+1, v) = \\sum_{s} \\alpha(k,s)\\cdot \\exp \\left[g_{k+1}(s,v)\\right]\n",
    "$$\n",
    "\n",
    "最终\n",
    "\n",
    "$$\n",
    "Z(\\bar{x}, w) = \\sum_{v}^{m} \\alpha(\\mathrm{T}, v) \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Computation（后向算法）\n",
    "\n",
    "$$\n",
    "\\beta(u, k) = \\sum_{s} \\left[ \\exp g_{k+1}(u,s) \\right]\\cdot \\beta(s,k+1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 概率值计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 计算边缘概率\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(y_{k}=u \\mid \\bar{x};w) &= \\frac{\\alpha(k,u)\\beta(u,k)}{Z(\\bar{x},w)} \\\\\n",
    "Z(\\bar{x},w) &= \\sum_{u} \\alpha(k,u)\\beta(u,k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- 计算相邻两个隐状态的联合概率\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(y_{k}=u, ~ y_{k+1} = v \\mid \\bar{x};w) &= \\frac{\\alpha(k,u)\\left[ \\exp g_{k+1}(u,v) \\right]\\beta(v,k+1)}{Z(\\bar{x},w)}  \\tag{7}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 CRF Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式（5）得到了一般log-linear model的参数估计形式，现在考虑CRF的参数估计。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial w_{j}} \\log P(\\bar{y} \\mid \\bar{x};w) &= F_{j}(\\bar{x}, \\bar{y}) - \\mathbb{E}_{y' \\sim P(y' | \\bar{x};w)} \\left[ F_{j}(\\bar{x}, y') \\right] \\\\\n",
    "&= F_{j}(\\bar{x}, \\bar{y}) - \\mathbb{E}_{\\bar{y}} \\left[ \\sum_{i=2}^{T}f_{j}(y_{i-1}, y_{i}, \\bar{x}) \\right] \\\\\n",
    "&= F_{j}(\\bar{x}, \\bar{y}) - \\sum_{i=2}^{T} \\mathbb{E}_{\\bar{y}} \\left[ f_{j}(y_{i-1}, y_{i}, \\bar{x}) \\right] \\\\\n",
    "&= F_{j}(\\bar{x}, \\bar{y}) - \\sum_{i=2}^{T} \\mathbb{E}_{y_{i-1}~,y_{i}} \\left[ f_{j}(y_{i-1}, y_{i}, \\bar{x}) \\right] \\\\\n",
    "&= F_{j}(\\bar{x}, \\bar{y}) - \\sum_{i=2}^{T} \\sum_{y_{i-1}} \\sum_{y_{i}} f_{j}(y_{i-1}, y_{i}, \\bar{x}) \\cdot P(y_{i-1}, y_{i}\\mid \\bar{x}; w)  \\tag{8}\\\\ \n",
    "&= F_{j}(\\bar{x}, \\bar{y}) - \\sum_{i=2}^{T} \\sum_{y_{i-1}} \\sum_{y_{i}} f_{j}(y_{i-1}, y_{i}, \\bar{x}) \\cdot \\frac{\\alpha(i-1, y_{i-1})\\left[ \\exp g_{i}(y_{i-1}, y_{i}) \\right]\\beta(y_{i}, i)}{Z(\\bar{x},w)}  \\tag{9}\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
