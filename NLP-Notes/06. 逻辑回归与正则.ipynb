{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Outline\n",
    "\n",
    "1. 生成模型 vs. 判别模型\n",
    "\n",
    "2. LR和最大似然\n",
    "\n",
    "3. L1 vs. L2 正则\n",
    "\n",
    "4. 交叉验证\n",
    "\n",
    "5. 正则的灵活使用\n",
    "\n",
    "6. MLE vs. MAP\n",
    "\n",
    "7. 特征选择与LASSO\n",
    "\n",
    "8. 坐标下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 生成模型 vs. 判别模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 字面上的区别\n",
    "\n",
    "$$\n",
    "\\begin{array}\n",
    "\\text{Generative Model:} \\quad \\boxed{Data} \\rightarrow \\boxed{Model} \\rightarrow \\boxed{New ~ Data}\\\\\n",
    "\\text{Discriminate Model:} \\quad \\boxed{Data} \\rightarrow \\boxed{Model} \\rightarrow \\boxed{判别}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "生成模型：Naive Bayes, HMM, VAE, GAN\n",
    "\n",
    "判别模型：LR, CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 训练模型的目标函数\n",
    "\n",
    "生成： Maximize $P(X,Y)$\n",
    "\n",
    "判别： Maximize $P(Y|X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 生成模型生成数据的原理\n",
    "\n",
    "生成: $P(x,y)=\\color{red}{P(x|y)}\\cdot P(y)$\n",
    "\n",
    "判别: $P(y|x)$\n",
    "\n",
    "其中，$\\color{red}{P(x|y)}$反应了数据的分布情况，从中采样可得到新的样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 生成模型用于判别\n",
    "\n",
    "生成: $P(x,y)=P(y|x)P(x)$  \n",
    "\n",
    "判别: $P(y|x)$\n",
    "\n",
    "对于分类问题，通常判别模型要优于生成模型；而当数据量少时，生成模型可能优于判别模型（因为$P(x)$是先验知识）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{eqnarray}\n",
    "P(y=1|x) &=& \\sigma(w^{\\mathrm{T}}x + b )  \\tag{1}\\\\ \n",
    "P(y=0|x) &=& 1-\\sigma(w^{\\mathrm{T}}x + b ) \\tag{2}\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可改写成\n",
    "\n",
    "$$\n",
    "P(y|x,w)=P(y=1|x,w)^{y}\\left[1-P(y=1|x,w)\\right]^{1-y} \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数$w,b$可通过MLE进行估计:\n",
    "\n",
    "$$\n",
    "\\hat{w}, \\hat{b} = \\arg \\max_{w,b} \\prod_{i=1}^{n}p(y_{i} | x_{i}; w, b) \\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\arg \\min &~& -\\sum_{i=1}^{n} \\log p(y_{i}|x_{i};w,b) \\\\\n",
    "&=& -\\sum_{i=1}^{n} \\log \\left[ P(y_{i}=1|x_{i},w,b)^{y_{i}}\\left[1-P(y_{i}=1|x_{i},w,b)\\right]^{1-y_{i}} \\right] \\\\\n",
    "&=& -\\sum_{i=1}^{n} y_{i} \\log \\sigma(w^{\\mathrm{T}}x_{i}+b) + (1-y_{i}) \\log \\left[1- \\sigma(w^{\\mathrm{T}}x_{i}+b)\\right] \\\\\n",
    "&=& L(w,b) \\tag{5}\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分别对$w,x$计算偏导\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L(w,b)}{\\partial w} &=& \\sum_{i=1}^{n} \\left[\\sigma(w^{\\mathrm{T}}x_{i}+b) - y_{i} \\right] \\cdot x_{i} \\tag{6} \\\\\n",
    "\\frac{\\partial L(w,b)}{\\partial b} &=& \\sum_{i=1}^{n} \\left[\\sigma(w^{\\mathrm{T}}x_{i}+b) - y_{i} \\right] \\tag{7}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "可使用梯度下降法或拟牛顿法求出$~\\hat{w}, \\hat{b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{l}\n",
    "for \\quad t=1,2, \\cdots, converge: \\\\\n",
    "\\quad \\quad w^{(t)} = w^{(t-1)} - \\eta_{t}\\cdot \\nabla_{w}L(w,b)\\\\\n",
    "\\quad \\quad b^{(t)} = b^{(t-1)} - \\eta_{t}\\cdot \\nabla_{b}L(w,b)\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Empirical Gradient: 式(6)(7)中用样本计算的梯度 \n",
    "\n",
    "- True Gradient\n",
    "\n",
    "\n",
    "- GD: 遍历所有样本计算gradient（比较慢）\n",
    "\n",
    "- SGD: 每次随机选取one sample计算gradient\n",
    "\n",
    "- Mini-Batch: 使用部分样本计算gradient（推荐）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 正则化的LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Overfitting\n",
    "- More Data\n",
    "- Regularization\n",
    "- Ensemble\n",
    "- less complicated model\n",
    "- less feature\n",
    "- add noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 范数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若$w$是向量:\n",
    "\n",
    "1-范数: $\\|w \\|_{1} = \\sum_{i=1}^{d}|w_{i}|$ \n",
    "\n",
    "2-范数: $\\|w \\|_{2} = \\sqrt{\\sum_{i=1}^{d}w_{i}^{2}}$\n",
    "\n",
    "若$w$是矩阵:\n",
    "\n",
    "1-范数: $\\|w\\|_{1} = \\max_{j}\\sum_{i=1}^{m}|w_{ij}|$\n",
    "\n",
    "2-范数: $\\|w\\|_{2} = \\sqrt{\\lambda_{1}}$\n",
    "\n",
    "F-范数: $\\|w\\|_{F} = \\left(\\sum_{i=1}^{m}\\sum_{j=1}^{n} w_{ij}^{2}\\right)^{1/2}$\n",
    "\n",
    "L2正则，指向量的2-范数，或矩阵的F-范数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1正则的Logistic Regression: （Lasso）\n",
    "\n",
    "$$\n",
    "\\hat{w}, \\hat{b} = \\arg \\min_{w,b} ~ -\\sum_{i=1}^{n} \\log p(y_{i}|x_{i};w,b) + \\lambda \\|w \\|_{1} \\\\\n",
    "$$\n",
    "\n",
    "L2正则的Logistic Regression: （Ridge Regression）\n",
    "\n",
    "$$\n",
    "\\hat{w}, \\hat{b} = \\arg \\min_{w,b} ~ -\\sum_{i=1}^{n} \\log p(y_{i}|x_{i};w,b) + \\lambda \\|w \\|_{2}^{\\color{red}{2}} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. L1 vs. L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "都能使得$~w~$变小，但L1正则学得的$~w~$是稀疏的，可以用于特征选择。\n",
    "\n",
    "比如：\n",
    "\n",
    "L1正则学得的$w=(0, ~0,~ 0.2,~ 0,~ \\cdots,~ 0.7)$\n",
    "\n",
    "L2正则学得的$w=(0.01, ~0.02,~ 0.2,~ 0,~ \\cdots,~ 0.7)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设$w=(w_{1}, w_{2})\\in \\mathbf{R}^{2}$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x) &= L(w,b) + \\lambda \\|w\\|_{1} \\\\\n",
    "f(x) &= L(w,b) + \\lambda \\|w\\|_{2}^{2}\n",
    "\\end{align}\n",
    "$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<talbe>\n",
    "    <td><img src='attachment/06. L1正则.png' style='zoom:50%'/></td>\n",
    "    <td><img src='attachment/06. L2正则.png' style='zoom:50%'/></td>\n",
    "</talbe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从图中可以看出，当$w$的维度较高时，L1正则的loss与regularization有较大概率在0点相交。而L2正则相交到0点的概率则小了很多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1正则特征稀疏的好处:  \n",
    "\n",
    "1. 如果维度太高，计算量也变得很高\n",
    "\n",
    "2. 在稀疏条件下，计算量只依赖非0项\n",
    "\n",
    "3. 提高可解释性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Lasso（L1正则的Linear Regression）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(w,b) = \\sum_{i=1}^{n} \\left(\\sum_{j=1}^{d}w_{j}x_{ij} +  b - y_{i} \\right)^{2} + \\lambda \\sum_{j=1}^{d}|w_{j}|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以计算$w_{l}$的最优值为例说明为什么L1正则引起稀疏解。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial w_{l}} &= 2 \\sum_{i=1}^{n} \\left(\\sum_{j=1}^{d}w_{j}x_{ij} +  b - y_{i} \\right) \\cdot x_{ij} + \\lambda \\frac{\\partial |w_{l}|}{\\partial w_{l}} \\\\\n",
    "&= 2 \\sum_{i=1}^{n} \\left(\\sum_{j \\neq 1}^{d}w_{j}x_{ij} +  b - y_{i} + w_{l}x_{il} \\right) \\cdot x_{ij} + \\lambda \\frac{\\partial |w_{l}|}{\\partial w_{l}} \\\\\n",
    "&= 2 \\sum_{i=1}^{n} \\left(\\sum_{j \\neq l}^{d}w_{j}x_{ij} +  b - y_{i}  \\right) \\cdot x_{ij} + 2\\sum_{i=1}^{n} w_{l}x_{il}^{2} + \\lambda \\frac{\\partial |w_{l}|}{\\partial w_{l}} \\\\\n",
    "&= 2 \\sum_{i=1}^{n} \\left(\\sum_{j \\neq l}^{d}w_{j}x_{ij} +  b - y_{i}  \\right) \\cdot x_{ij} + w_{l}\\cdot \\sum_{i=1}^{n} 2x_{il}^{2} + \\lambda \\frac{\\partial |w_{l}|}{\\partial w_{l}} \\\\\n",
    "&= C_{l} + w_{l}\\cdot a_{l} + \\lambda \\frac{\\partial |w_{l}|}{\\partial w_{l}} \\\\\n",
    "&= \\left\\{\\begin{matrix}\n",
    " C_{l} + w_{l}\\cdot a_{l} + \\lambda , & w_{l} > 0\\\\\n",
    " \\left[C_{l} - \\lambda, C_{l} + \\lambda \\right], & w_{l} > 0 \\\\\n",
    " C_{l} + w_{l}\\cdot a_{l} -  \\lambda, & w_{l} < 0\n",
    "\\end{matrix}\\right.\n",
    "\\end{align} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令$\\frac{\\partial L}{\\partial w_{l}} = 0$\n",
    "\n",
    "当$w_{l}>0$时: \n",
    "\n",
    "$$\n",
    "C_{l} + w_{l}\\cdot a_{l} + \\lambda = 0 \\\\\n",
    "\\color{blue}{w_{l} = \\frac{-C_{l}-\\lambda}{a_{l}}} \\\\\n",
    "\\because a_{l} = \\sum_{i=1}^{n}2x_{il}^{2} > 0 \\\\\n",
    "\\therefore -C_{l}-\\lambda > 0 \\\\\n",
    "\\therefore  \\color{blue}{C_{l} < - \\lambda}\n",
    "$$\n",
    "\n",
    "当$w_{l}<0$时: \n",
    "\n",
    "$$\n",
    "C_{l} + w_{l}\\cdot a_{l} - \\lambda = 0 \\\\\n",
    "\\color{blue}{w_{l} = \\frac{-C_{l} + \\lambda}{a_{l}}} \\\\\n",
    "\\because a_{l} = \\sum_{i=1}^{n}2x_{il}^{2} > 0 \\\\\n",
    "\\therefore -C_{l} + \\lambda < 0 \\\\\n",
    "\\therefore \\color{blue}{C_{l} >  \\lambda}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终得到$w_{l}$的最优解为: \n",
    "\n",
    "$$\n",
    "\\hat{w_{l}} = \\left\\{\\begin{matrix}\n",
    " \\frac{-C_{l}-\\lambda}{a_{l}}, & if \\quad C_{l} < -\\lambda \\\\\n",
    " \\color{red}{0}, & if \\color{red}{\\quad -\\lambda \\leq C_{l} \\leq \\lambda} \\Rightarrow \\color{red}{sparcity} \\\\\n",
    "\\frac{-C_{l} + \\lambda}{a_{l}} , & if \\quad C_{l} > \\lambda\n",
    "\\end{matrix}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Ridge（L2正则的Linear Regression）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L = \\sum_{i=1}^{n} \\left(w^{\\mathrm{T}}x_{i} - y_{i} \\right)^{2} + \\lambda \\|w \\|_{2}^{2}\n",
    "$$\n",
    "\n",
    "不带正则的Linear Regression的解为 $w=(x^{\\mathrm{T}}x)^{-1}x^{\\mathrm{T}}y$\n",
    "\n",
    "L2正则的Linear Regression的解为 $w=(x^{\\mathrm{T}}x + \\lambda I)^{-1}x^{\\mathrm{T}}y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
